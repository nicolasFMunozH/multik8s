* 
* ==> Audit <==
* |-----------|---------------------------|----------|------|---------|---------------------|---------------------|
|  Command  |           Args            | Profile  | User | Version |     Start Time      |      End Time       |
|-----------|---------------------------|----------|------|---------|---------------------|---------------------|
| start     |                           | minikube | root | v1.29.0 | 06 Mar 23 18:47 -03 |                     |
| start     |                           | minikube | root | v1.29.0 | 06 Mar 23 18:48 -03 |                     |
| start     | --driver=docker           | minikube | root | v1.29.0 | 06 Mar 23 19:18 -03 |                     |
| start     |                           | minikube | root | v1.29.0 | 06 Mar 23 19:18 -03 |                     |
| start     |                           | minikube | root | v1.29.0 | 06 Mar 23 19:18 -03 |                     |
| start     | --force                   | minikube | root | v1.29.0 | 06 Mar 23 19:19 -03 | 06 Mar 23 19:23 -03 |
| start     |                           | minikube | root | v1.29.0 | 06 Mar 23 19:23 -03 |                     |
| start     | --driver=docker           | minikube | root | v1.29.0 | 06 Mar 23 19:23 -03 |                     |
| start     | --driver=docker --force   | minikube | root | v1.29.0 | 06 Mar 23 19:24 -03 | 06 Mar 23 19:25 -03 |
| start     | --driver=docker --force   | minikube | root | v1.29.0 | 06 Mar 23 19:30 -03 | 06 Mar 23 19:31 -03 |
| ip        |                           | minikube | root | v1.29.0 | 06 Mar 23 19:35 -03 | 06 Mar 23 19:35 -03 |
| service   | client-deployment         | minikube | root | v1.29.0 | 06 Mar 23 19:47 -03 | 06 Mar 23 19:48 -03 |
| service   | client-deployment         | minikube | root | v1.29.0 | 06 Mar 23 19:51 -03 | 06 Mar 23 19:51 -03 |
| ip        |                           | minikube | root | v1.29.0 | 06 Mar 23 19:53 -03 | 06 Mar 23 19:53 -03 |
| kubectl   | delete ingress myingress  | minikube | root | v1.29.0 | 06 Mar 23 20:06 -03 | 06 Mar 23 20:06 -03 |
| addons    | enable ingress            | minikube | root | v1.29.0 | 06 Mar 23 20:10 -03 | 06 Mar 23 20:11 -03 |
| ip        |                           | minikube | root | v1.29.0 | 06 Mar 23 20:15 -03 | 06 Mar 23 20:15 -03 |
| dashboard |                           | minikube | root | v1.29.0 | 06 Mar 23 20:19 -03 |                     |
| service   | client-deployment         | minikube | root | v1.29.0 | 06 Mar 23 20:22 -03 | 06 Mar 23 20:23 -03 |
| addons    | enable metrics-server     | minikube | root | v1.29.0 | 06 Mar 23 20:23 -03 | 06 Mar 23 20:23 -03 |
| service   | client-deployment         | minikube | root | v1.29.0 | 06 Mar 23 20:24 -03 | 06 Mar 23 20:26 -03 |
| service   | client-deployment         | minikube | root | v1.29.0 | 06 Mar 23 20:27 -03 | 06 Mar 23 21:07 -03 |
| stop      |                           | minikube | root | v1.29.0 | 06 Mar 23 22:59 -03 | 06 Mar 23 22:59 -03 |
| start     | --driver=docker --force   | minikube | root | v1.29.0 | 07 Mar 23 09:01 -03 | 07 Mar 23 09:02 -03 |
| ip        |                           | minikube | root | v1.29.0 | 07 Mar 23 10:38 -03 | 07 Mar 23 10:38 -03 |
| service   |                           | minikube | root | v1.29.0 | 07 Mar 23 10:38 -03 |                     |
| service   | client-cluster-ip-service | minikube | root | v1.29.0 | 07 Mar 23 10:39 -03 | 07 Mar 23 10:39 -03 |
| stop      |                           | minikube | root | v1.29.0 | 07 Mar 23 11:37 -03 | 07 Mar 23 11:38 -03 |
| start     | --driver=docker --force   | minikube | root | v1.29.0 | 07 Mar 23 18:38 -03 | 07 Mar 23 18:40 -03 |
| addons    | enable ingress            | minikube | root | v1.29.0 | 07 Mar 23 21:04 -03 | 07 Mar 23 21:04 -03 |
| addons    | enable ingress            | minikube | root | v1.29.0 | 07 Mar 23 22:23 -03 | 07 Mar 23 22:23 -03 |
| addons    | enable ingress            | minikube | root | v1.29.0 | 07 Mar 23 22:33 -03 |                     |
| addons    | enable ingress            | minikube | root | v1.29.0 | 07 Mar 23 22:36 -03 | 07 Mar 23 22:36 -03 |
| addons    | enable ingress            | minikube | root | v1.29.0 | 07 Mar 23 22:38 -03 | 07 Mar 23 22:38 -03 |
| addons    | enable ingress            | minikube | root | v1.29.0 | 07 Mar 23 22:46 -03 |                     |
|-----------|---------------------------|----------|------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2023/03/07 18:38:44
Running on machine: APIUXNB1515
Binary: Built with gc go1.19.5 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0307 18:38:44.594152    2327 out.go:296] Setting OutFile to fd 1 ...
I0307 18:38:44.595090    2327 out.go:348] isatty.IsTerminal(1) = true
I0307 18:38:44.595098    2327 out.go:309] Setting ErrFile to fd 2...
I0307 18:38:44.595106    2327 out.go:348] isatty.IsTerminal(2) = true
I0307 18:38:44.596004    2327 root.go:334] Updating PATH: /root/.minikube/bin
W0307 18:38:44.596583    2327 root.go:311] Error reading config file at /root/.minikube/config/config.json: open /root/.minikube/config/config.json: no such file or directory
I0307 18:38:44.598705    2327 out.go:303] Setting JSON to false
I0307 18:38:44.603504    2327 start.go:125] hostinfo: {"hostname":"APIUXNB1515","uptime":34687,"bootTime":1678190438,"procs":24,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"22.04","kernelVersion":"5.10.16.3-microsoft-standard-WSL2","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"4147feb8-7c82-47d0-afbc-bca03f53a0cd"}
I0307 18:38:44.603635    2327 start.go:135] virtualization:  
I0307 18:38:44.618145    2327 out.go:177] 😄  minikube v1.29.0 on Ubuntu 22.04
W0307 18:38:44.632028    2327 out.go:239] ❗  minikube skips various validations when --force is supplied; this may lead to unexpected behavior
I0307 18:38:44.633292    2327 config.go:180] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.26.1
I0307 18:38:44.635261    2327 notify.go:220] Checking for updates...
I0307 18:38:44.640119    2327 driver.go:365] Setting default libvirt URI to qemu:///system
I0307 18:38:44.792944    2327 docker.go:141] docker version: linux-20.10.22:Docker Desktop
I0307 18:38:44.793232    2327 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0307 18:38:46.046107    2327 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (1.2528542s)
I0307 18:38:46.048223    2327 info.go:266] docker info: {ID:3MIN:WLHE:BMAW:7LDF:CGUE:JQSG:47LZ:UAC3:TRJL:64BC:4BW5:I3OU Containers:44 ContainersRunning:14 ContainersPaused:12 ContainersStopped:18 Images:101 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:true KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:292 OomKillDisable:true NGoroutines:246 SystemTime:2023-03-07 21:38:44.8266044 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:9 KernelVersion:5.10.16.3-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:13298556928 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:20.10.22 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:9ba4b250366a5ddde94bb7c9d1def331423aa323 Expected:9ba4b250366a5ddde94bb7c9d1def331423aa323} RuncCommit:{ID:v1.1.4-0-g5fd4c4d Expected:v1.1.4-0-g5fd4c4d} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=default] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.10.0] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.15.1] map[Name:dev Path:/usr/local/lib/docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.0.5] map[Name:extension Path:/usr/local/lib/docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.17] map[Name:sbom Path:/usr/local/lib/docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:/usr/local/lib/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.23.0]] Warnings:<nil>}}
I0307 18:38:46.049000    2327 docker.go:282] overlay module found
I0307 18:38:46.054866    2327 out.go:177] ✨  Using the docker driver based on existing profile
I0307 18:38:46.059552    2327 start.go:296] selected driver: docker
I0307 18:38:46.059776    2327 start.go:857] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.37@sha256:8bf7a0e8a062bc5e2b71d28b35bfa9cc862d9220e234e86176b3785f685d8b15 Memory:3100 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.26.1 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.26.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true ingress:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/root:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP:}
I0307 18:38:46.059958    2327 start.go:868] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
W0307 18:38:46.061355    2327 out.go:239] 🛑  The "docker" driver should not be used with root privileges. If you wish to continue as root, use --force.
W0307 18:38:46.061446    2327 out.go:239] 💡  If you are running minikube within a VM, consider using --driver=none:
W0307 18:38:46.061548    2327 out.go:239] 📘    https://minikube.sigs.k8s.io/docs/reference/drivers/none/
I0307 18:38:46.061837    2327 config.go:180] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.26.1
W0307 18:38:46.061944    2327 out.go:239] 💡  Tip: To remove this root owned cluster, run: sudo minikube delete
I0307 18:38:46.062039    2327 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0307 18:38:46.227659    2327 info.go:266] docker info: {ID:3MIN:WLHE:BMAW:7LDF:CGUE:JQSG:47LZ:UAC3:TRJL:64BC:4BW5:I3OU Containers:44 ContainersRunning:14 ContainersPaused:12 ContainersStopped:18 Images:101 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:true KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:292 OomKillDisable:true NGoroutines:245 SystemTime:2023-03-07 21:38:46.1039749 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:9 KernelVersion:5.10.16.3-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:13298556928 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:20.10.22 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:9ba4b250366a5ddde94bb7c9d1def331423aa323 Expected:9ba4b250366a5ddde94bb7c9d1def331423aa323} RuncCommit:{ID:v1.1.4-0-g5fd4c4d Expected:v1.1.4-0-g5fd4c4d} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=default] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.10.0] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.15.1] map[Name:dev Path:/usr/local/lib/docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.0.5] map[Name:extension Path:/usr/local/lib/docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.17] map[Name:sbom Path:/usr/local/lib/docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:/usr/local/lib/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.23.0]] Warnings:<nil>}}
I0307 18:38:46.231015    2327 cni.go:84] Creating CNI manager for ""
I0307 18:38:46.231599    2327 cni.go:157] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0307 18:38:46.231933    2327 start_flags.go:319] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.37@sha256:8bf7a0e8a062bc5e2b71d28b35bfa9cc862d9220e234e86176b3785f685d8b15 Memory:3100 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.26.1 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.26.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true ingress:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/root:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP:}
I0307 18:38:46.237835    2327 out.go:177] 👍  Starting control plane node minikube in cluster minikube
I0307 18:38:46.244414    2327 cache.go:120] Beginning downloading kic base image for docker with docker
I0307 18:38:46.253669    2327 out.go:177] 🚜  Pulling base image ...
I0307 18:38:46.259665    2327 preload.go:132] Checking if preload exists for k8s version v1.26.1 and runtime docker
I0307 18:38:46.259962    2327 preload.go:148] Found local preload: /root/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.26.1-docker-overlay2-amd64.tar.lz4
I0307 18:38:46.259971    2327 cache.go:57] Caching tarball of preloaded images
I0307 18:38:46.260316    2327 image.go:77] Checking for gcr.io/k8s-minikube/kicbase:v0.0.37@sha256:8bf7a0e8a062bc5e2b71d28b35bfa9cc862d9220e234e86176b3785f685d8b15 in local docker daemon
I0307 18:38:46.262034    2327 preload.go:174] Found /root/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.26.1-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0307 18:38:46.262084    2327 cache.go:60] Finished verifying existence of preloaded tar for  v1.26.1 on docker
I0307 18:38:46.262218    2327 profile.go:148] Saving config to /root/.minikube/profiles/minikube/config.json ...
I0307 18:38:46.320881    2327 image.go:81] Found gcr.io/k8s-minikube/kicbase:v0.0.37@sha256:8bf7a0e8a062bc5e2b71d28b35bfa9cc862d9220e234e86176b3785f685d8b15 in local docker daemon, skipping pull
I0307 18:38:46.320928    2327 cache.go:143] gcr.io/k8s-minikube/kicbase:v0.0.37@sha256:8bf7a0e8a062bc5e2b71d28b35bfa9cc862d9220e234e86176b3785f685d8b15 exists in daemon, skipping load
I0307 18:38:46.321244    2327 cache.go:193] Successfully downloaded all kic artifacts
I0307 18:38:46.321820    2327 start.go:364] acquiring machines lock for minikube: {Name:mke11f63b5835bf422927bf558fccac7a21a838f Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0307 18:38:46.321949    2327 start.go:368] acquired machines lock for "minikube" in 86.9µs
I0307 18:38:46.322024    2327 start.go:96] Skipping create...Using existing machine configuration
I0307 18:38:46.322641    2327 fix.go:55] fixHost starting: 
I0307 18:38:46.323085    2327 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0307 18:38:46.378819    2327 fix.go:103] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0307 18:38:46.378853    2327 fix.go:129] unexpected machine state, will restart: <nil>
I0307 18:38:46.385264    2327 out.go:177] 🔄  Restarting existing docker container for "minikube" ...
I0307 18:38:46.391426    2327 cli_runner.go:164] Run: docker start minikube
I0307 18:38:49.088546    2327 cli_runner.go:217] Completed: docker start minikube: (2.6970744s)
I0307 18:38:49.088932    2327 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0307 18:38:49.162597    2327 kic.go:426] container "minikube" state is running.
I0307 18:38:49.167855    2327 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0307 18:38:49.249301    2327 profile.go:148] Saving config to /root/.minikube/profiles/minikube/config.json ...
I0307 18:38:49.254259    2327 machine.go:88] provisioning docker machine ...
I0307 18:38:49.256748    2327 ubuntu.go:169] provisioning hostname "minikube"
I0307 18:38:49.257415    2327 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0307 18:38:49.354582    2327 main.go:141] libmachine: Using SSH client type: native
I0307 18:38:49.360515    2327 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x7f1980] 0x7f4b00 <nil>  [] 0s} 127.0.0.1 53896 <nil> <nil>}
I0307 18:38:49.360544    2327 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0307 18:38:49.383322    2327 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I0307 18:38:52.589286    2327 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0307 18:38:52.589887    2327 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0307 18:38:52.647977    2327 main.go:141] libmachine: Using SSH client type: native
I0307 18:38:52.648139    2327 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x7f1980] 0x7f4b00 <nil>  [] 0s} 127.0.0.1 53896 <nil> <nil>}
I0307 18:38:52.648172    2327 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0307 18:38:52.808605    2327 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0307 18:38:52.808969    2327 ubuntu.go:175] set auth options {CertDir:/root/.minikube CaCertPath:/root/.minikube/certs/ca.pem CaPrivateKeyPath:/root/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/root/.minikube/machines/server.pem ServerKeyPath:/root/.minikube/machines/server-key.pem ClientKeyPath:/root/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/root/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/root/.minikube}
I0307 18:38:52.809004    2327 ubuntu.go:177] setting up certificates
I0307 18:38:52.809414    2327 provision.go:83] configureAuth start
I0307 18:38:52.809488    2327 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0307 18:38:52.862450    2327 provision.go:138] copyHostCerts
I0307 18:38:52.863573    2327 exec_runner.go:144] found /root/.minikube/ca.pem, removing ...
I0307 18:38:52.863891    2327 exec_runner.go:207] rm: /root/.minikube/ca.pem
I0307 18:38:52.864866    2327 exec_runner.go:151] cp: /root/.minikube/certs/ca.pem --> /root/.minikube/ca.pem (1078 bytes)
I0307 18:38:52.865743    2327 exec_runner.go:144] found /root/.minikube/cert.pem, removing ...
I0307 18:38:52.865751    2327 exec_runner.go:207] rm: /root/.minikube/cert.pem
I0307 18:38:52.865797    2327 exec_runner.go:151] cp: /root/.minikube/certs/cert.pem --> /root/.minikube/cert.pem (1119 bytes)
I0307 18:38:52.865951    2327 exec_runner.go:144] found /root/.minikube/key.pem, removing ...
I0307 18:38:52.865957    2327 exec_runner.go:207] rm: /root/.minikube/key.pem
I0307 18:38:52.866034    2327 exec_runner.go:151] cp: /root/.minikube/certs/key.pem --> /root/.minikube/key.pem (1679 bytes)
I0307 18:38:52.866229    2327 provision.go:112] generating server cert: /root/.minikube/machines/server.pem ca-key=/root/.minikube/certs/ca.pem private-key=/root/.minikube/certs/ca-key.pem org=xinamo.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I0307 18:38:53.107584    2327 provision.go:172] copyRemoteCerts
I0307 18:38:53.108170    2327 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0307 18:38:53.108250    2327 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0307 18:38:53.162937    2327 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53896 SSHKeyPath:/root/.minikube/machines/minikube/id_rsa Username:docker}
I0307 18:38:53.284835    2327 ssh_runner.go:362] scp /root/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1078 bytes)
I0307 18:38:53.320928    2327 ssh_runner.go:362] scp /root/.minikube/machines/server.pem --> /etc/docker/server.pem (1200 bytes)
I0307 18:38:53.353060    2327 ssh_runner.go:362] scp /root/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0307 18:38:53.386945    2327 provision.go:86] duration metric: configureAuth took 576.8424ms
I0307 18:38:53.386971    2327 ubuntu.go:193] setting minikube options for container-runtime
I0307 18:38:53.387584    2327 config.go:180] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.26.1
I0307 18:38:53.387650    2327 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0307 18:38:53.480776    2327 main.go:141] libmachine: Using SSH client type: native
I0307 18:38:53.480987    2327 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x7f1980] 0x7f4b00 <nil>  [] 0s} 127.0.0.1 53896 <nil> <nil>}
I0307 18:38:53.480997    2327 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0307 18:38:53.657556    2327 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0307 18:38:53.657572    2327 ubuntu.go:71] root file system type: overlay
I0307 18:38:53.658226    2327 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0307 18:38:53.658801    2327 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0307 18:38:53.766182    2327 main.go:141] libmachine: Using SSH client type: native
I0307 18:38:53.770372    2327 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x7f1980] 0x7f4b00 <nil>  [] 0s} 127.0.0.1 53896 <nil> <nil>}
I0307 18:38:53.770488    2327 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0307 18:38:53.994370    2327 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0307 18:38:53.994866    2327 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0307 18:38:54.144984    2327 main.go:141] libmachine: Using SSH client type: native
I0307 18:38:54.149946    2327 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x7f1980] 0x7f4b00 <nil>  [] 0s} 127.0.0.1 53896 <nil> <nil>}
I0307 18:38:54.149979    2327 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0307 18:38:54.339309    2327 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0307 18:38:54.339324    2327 machine.go:91] provisioned docker machine in 5.0850455s
I0307 18:38:54.339336    2327 start.go:300] post-start starting for "minikube" (driver="docker")
I0307 18:38:54.339343    2327 start.go:328] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0307 18:38:54.339417    2327 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0307 18:38:54.339474    2327 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0307 18:38:54.433149    2327 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53896 SSHKeyPath:/root/.minikube/machines/minikube/id_rsa Username:docker}
I0307 18:38:54.589356    2327 ssh_runner.go:195] Run: cat /etc/os-release
I0307 18:38:54.595714    2327 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0307 18:38:54.620061    2327 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0307 18:38:54.620092    2327 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0307 18:38:54.620181    2327 info.go:137] Remote host: Ubuntu 20.04.5 LTS
I0307 18:38:54.620256    2327 filesync.go:126] Scanning /root/.minikube/addons for local assets ...
I0307 18:38:54.620375    2327 filesync.go:126] Scanning /root/.minikube/files for local assets ...
I0307 18:38:54.621917    2327 start.go:303] post-start completed in 282.5646ms
I0307 18:38:54.621999    2327 fix.go:57] fixHost completed within 8.2998915s
I0307 18:38:54.622004    2327 start.go:83] releasing machines lock for "minikube", held for 8.3000496s
I0307 18:38:54.622073    2327 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0307 18:38:54.702311    2327 ssh_runner.go:195] Run: cat /version.json
I0307 18:38:54.702388    2327 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0307 18:38:54.702728    2327 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0307 18:38:54.703065    2327 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0307 18:38:54.781131    2327 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53896 SSHKeyPath:/root/.minikube/machines/minikube/id_rsa Username:docker}
I0307 18:38:54.787985    2327 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53896 SSHKeyPath:/root/.minikube/machines/minikube/id_rsa Username:docker}
I0307 18:38:56.155073    2327 ssh_runner.go:235] Completed: cat /version.json: (1.4527342s)
I0307 18:38:56.155450    2327 ssh_runner.go:195] Run: systemctl --version
I0307 18:38:56.155556    2327 ssh_runner.go:235] Completed: curl -sS -m 2 https://registry.k8s.io/: (1.4528109s)
I0307 18:38:56.248936    2327 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0307 18:38:56.337573    2327 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0307 18:38:56.463638    2327 cni.go:229] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0307 18:38:56.464697    2327 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0307 18:38:56.482501    2327 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (135 bytes)
I0307 18:38:56.503975    2327 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0307 18:38:56.518487    2327 cni.go:258] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0307 18:38:56.518830    2327 start.go:483] detecting cgroup driver to use...
I0307 18:38:56.518864    2327 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0307 18:38:56.519439    2327 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
image-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0307 18:38:56.540402    2327 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0307 18:38:56.555588    2327 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0307 18:38:56.572974    2327 containerd.go:145] configuring containerd to use "cgroupfs" as cgroup driver...
I0307 18:38:56.573048    2327 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0307 18:38:56.587196    2327 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0307 18:38:56.601248    2327 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0307 18:38:56.615836    2327 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0307 18:38:56.631164    2327 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0307 18:38:56.647276    2327 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0307 18:38:56.662237    2327 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0307 18:38:56.677441    2327 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0307 18:38:56.698466    2327 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0307 18:38:56.919509    2327 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0307 18:38:57.156617    2327 start.go:483] detecting cgroup driver to use...
I0307 18:38:57.166507    2327 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0307 18:38:57.166605    2327 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0307 18:38:57.219912    2327 cruntime.go:273] skipping containerd shutdown because we are bound to it
I0307 18:38:57.219991    2327 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0307 18:38:57.245582    2327 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
image-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0307 18:38:57.303157    2327 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0307 18:38:57.652375    2327 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0307 18:38:57.837407    2327 docker.go:529] configuring docker to use "cgroupfs" as cgroup driver...
I0307 18:38:57.837451    2327 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (144 bytes)
I0307 18:38:57.859506    2327 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0307 18:38:57.990663    2327 ssh_runner.go:195] Run: sudo systemctl restart docker
I0307 18:39:04.431844    2327 ssh_runner.go:235] Completed: sudo systemctl restart docker: (6.4411552s)
I0307 18:39:04.431928    2327 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0307 18:39:04.563936    2327 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0307 18:39:04.704471    2327 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0307 18:39:04.825983    2327 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0307 18:39:04.940320    2327 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0307 18:39:04.959571    2327 start.go:530] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0307 18:39:04.959650    2327 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0307 18:39:04.963376    2327 start.go:551] Will wait 60s for crictl version
I0307 18:39:04.964393    2327 ssh_runner.go:195] Run: which crictl
I0307 18:39:04.967854    2327 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0307 18:39:05.847816    2327 start.go:567] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  20.10.23
RuntimeApiVersion:  v1alpha2
I0307 18:39:05.847890    2327 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0307 18:39:06.134297    2327 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0307 18:39:06.184443    2327 out.go:204] 🐳  Preparing Kubernetes v1.26.1 on Docker 20.10.23 ...
I0307 18:39:06.185552    2327 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0307 18:39:06.228466    2327 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0307 18:39:06.232874    2327 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0307 18:39:06.247832    2327 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0307 18:39:06.307299    2327 preload.go:132] Checking if preload exists for k8s version v1.26.1 and runtime docker
I0307 18:39:06.307364    2327 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0307 18:39:06.343646    2327 docker.go:630] Got preloaded images: -- stdout --
redis:latest
postgres:latest
registry.k8s.io/kube-apiserver:v1.26.1
registry.k8s.io/kube-controller-manager:v1.26.1
registry.k8s.io/kube-scheduler:v1.26.1
registry.k8s.io/kube-proxy:v1.26.1
registry.k8s.io/metrics-server/metrics-server:<none>
registry.k8s.io/etcd:3.5.6-0
registry.k8s.io/ingress-nginx/controller:<none>
registry.k8s.io/pause:3.9
registry.k8s.io/ingress-nginx/kube-webhook-certgen:<none>
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
registry.k8s.io/coredns/coredns:v1.9.3
registry.k8s.io/pause:3.6
gcr.io/k8s-minikube/storage-provisioner:v5
stephengrider/multi-worker:latest
stephengrider/multi-server:latest
stephengrider/multi-client:latest

-- /stdout --
I0307 18:39:06.343655    2327 docker.go:560] Images already preloaded, skipping extraction
I0307 18:39:06.344130    2327 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0307 18:39:06.371535    2327 docker.go:630] Got preloaded images: -- stdout --
redis:latest
postgres:latest
registry.k8s.io/kube-apiserver:v1.26.1
registry.k8s.io/kube-controller-manager:v1.26.1
registry.k8s.io/kube-scheduler:v1.26.1
registry.k8s.io/kube-proxy:v1.26.1
registry.k8s.io/metrics-server/metrics-server:<none>
registry.k8s.io/etcd:3.5.6-0
registry.k8s.io/ingress-nginx/controller:<none>
registry.k8s.io/pause:3.9
registry.k8s.io/ingress-nginx/kube-webhook-certgen:<none>
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
registry.k8s.io/coredns/coredns:v1.9.3
registry.k8s.io/pause:3.6
gcr.io/k8s-minikube/storage-provisioner:v5
stephengrider/multi-worker:latest
stephengrider/multi-server:latest
stephengrider/multi-client:latest

-- /stdout --
I0307 18:39:06.371738    2327 cache_images.go:84] Images are preloaded, skipping loading
I0307 18:39:06.372201    2327 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0307 18:39:06.989252    2327 cni.go:84] Creating CNI manager for ""
I0307 18:39:06.989266    2327 cni.go:157] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0307 18:39:06.990819    2327 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0307 18:39:06.990926    2327 kubeadm.go:172] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.26.1 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m]}
I0307 18:39:06.991098    2327 kubeadm.go:177] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: /var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.26.1
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0307 18:39:06.992101    2327 kubeadm.go:968] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.26.1/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime=remote --container-runtime-endpoint=/var/run/cri-dockerd.sock --hostname-override=minikube --image-service-endpoint=/var/run/cri-dockerd.sock --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.26.1 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0307 18:39:06.992179    2327 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.26.1
I0307 18:39:07.012386    2327 binaries.go:44] Found k8s binaries, skipping transfer
I0307 18:39:07.012433    2327 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0307 18:39:07.021282    2327 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (440 bytes)
I0307 18:39:07.040540    2327 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0307 18:39:07.060108    2327 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2084 bytes)
I0307 18:39:07.078712    2327 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0307 18:39:07.083055    2327 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0307 18:39:07.099684    2327 certs.go:56] Setting up /root/.minikube/profiles/minikube for IP: 192.168.49.2
I0307 18:39:07.099732    2327 certs.go:186] acquiring lock for shared ca certs: {Name:mkb814c315fe9b7fabb439d6d58c5448fbb7853c Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0307 18:39:07.100224    2327 certs.go:195] skipping minikubeCA CA generation: /root/.minikube/ca.key
I0307 18:39:07.100398    2327 certs.go:195] skipping proxyClientCA CA generation: /root/.minikube/proxy-client-ca.key
I0307 18:39:07.100545    2327 certs.go:311] skipping minikube-user signed cert generation: /root/.minikube/profiles/minikube/client.key
I0307 18:39:07.100941    2327 certs.go:311] skipping minikube signed cert generation: /root/.minikube/profiles/minikube/apiserver.key.dd3b5fb2
I0307 18:39:07.101681    2327 certs.go:311] skipping aggregator signed cert generation: /root/.minikube/profiles/minikube/proxy-client.key
I0307 18:39:07.101942    2327 certs.go:401] found cert: /root/.minikube/certs/root/.minikube/certs/ca-key.pem (1679 bytes)
I0307 18:39:07.101965    2327 certs.go:401] found cert: /root/.minikube/certs/root/.minikube/certs/ca.pem (1078 bytes)
I0307 18:39:07.101985    2327 certs.go:401] found cert: /root/.minikube/certs/root/.minikube/certs/cert.pem (1119 bytes)
I0307 18:39:07.102017    2327 certs.go:401] found cert: /root/.minikube/certs/root/.minikube/certs/key.pem (1679 bytes)
I0307 18:39:07.105221    2327 ssh_runner.go:362] scp /root/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I0307 18:39:07.135113    2327 ssh_runner.go:362] scp /root/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0307 18:39:07.163878    2327 ssh_runner.go:362] scp /root/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0307 18:39:07.190653    2327 ssh_runner.go:362] scp /root/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0307 18:39:07.222165    2327 ssh_runner.go:362] scp /root/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0307 18:39:07.256482    2327 ssh_runner.go:362] scp /root/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0307 18:39:07.293489    2327 ssh_runner.go:362] scp /root/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0307 18:39:07.324892    2327 ssh_runner.go:362] scp /root/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0307 18:39:07.353784    2327 ssh_runner.go:362] scp /root/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0307 18:39:07.381168    2327 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0307 18:39:07.408138    2327 ssh_runner.go:195] Run: openssl version
I0307 18:39:07.447610    2327 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0307 18:39:07.493457    2327 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0307 18:39:07.506673    2327 certs.go:444] hashing: -rw-r--r-- 1 root root 1111 Mar  6 22:21 /usr/share/ca-certificates/minikubeCA.pem
I0307 18:39:07.506728    2327 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0307 18:39:07.521986    2327 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0307 18:39:07.540214    2327 kubeadm.go:401] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.37@sha256:8bf7a0e8a062bc5e2b71d28b35bfa9cc862d9220e234e86176b3785f685d8b15 Memory:3100 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.26.1 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.26.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true ingress:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/root:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP:}
I0307 18:39:07.540355    2327 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0307 18:39:07.710346    2327 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0307 18:39:07.805542    2327 kubeadm.go:416] found existing configuration files, will attempt cluster restart
I0307 18:39:07.805766    2327 kubeadm.go:633] restartCluster start
I0307 18:39:07.805868    2327 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0307 18:39:07.820369    2327 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0307 18:39:07.820433    2327 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0307 18:39:07.891760    2327 kubeconfig.go:135] verify returned: extract IP: "minikube" does not appear in /root/.kube/config
I0307 18:39:07.892032    2327 kubeconfig.go:146] "minikube" context is missing from /root/.kube/config - will repair!
I0307 18:39:07.892293    2327 lock.go:35] WriteFile acquiring /root/.kube/config: {Name:mk72a1487fd2da23da9e8181e16f352a6105bd56 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0307 18:39:07.906869    2327 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0307 18:39:07.917387    2327 api_server.go:165] Checking apiserver status ...
I0307 18:39:07.917429    2327 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0307 18:39:07.933312    2327 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0307 18:39:08.433819    2327 api_server.go:165] Checking apiserver status ...
I0307 18:39:08.433876    2327 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0307 18:39:08.446092    2327 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0307 18:39:08.934283    2327 api_server.go:165] Checking apiserver status ...
I0307 18:39:08.934363    2327 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0307 18:39:08.952035    2327 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0307 18:39:09.433927    2327 api_server.go:165] Checking apiserver status ...
I0307 18:39:09.433995    2327 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0307 18:39:09.455675    2327 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0307 18:39:09.933612    2327 api_server.go:165] Checking apiserver status ...
I0307 18:39:09.933742    2327 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0307 18:39:09.946881    2327 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0307 18:39:10.433843    2327 api_server.go:165] Checking apiserver status ...
I0307 18:39:10.433910    2327 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0307 18:39:10.446860    2327 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0307 18:39:10.933915    2327 api_server.go:165] Checking apiserver status ...
I0307 18:39:10.933983    2327 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0307 18:39:10.949962    2327 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0307 18:39:11.434403    2327 api_server.go:165] Checking apiserver status ...
I0307 18:39:11.434508    2327 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0307 18:39:11.446008    2327 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0307 18:39:11.934443    2327 api_server.go:165] Checking apiserver status ...
I0307 18:39:11.934557    2327 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0307 18:39:11.947378    2327 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0307 18:39:12.434073    2327 api_server.go:165] Checking apiserver status ...
I0307 18:39:12.434151    2327 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0307 18:39:12.449558    2327 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0307 18:39:12.934380    2327 api_server.go:165] Checking apiserver status ...
I0307 18:39:12.934558    2327 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0307 18:39:12.951250    2327 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0307 18:39:13.434223    2327 api_server.go:165] Checking apiserver status ...
I0307 18:39:13.434323    2327 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0307 18:39:13.446943    2327 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0307 18:39:13.934636    2327 api_server.go:165] Checking apiserver status ...
I0307 18:39:13.935095    2327 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0307 18:39:13.978221    2327 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0307 18:39:14.433762    2327 api_server.go:165] Checking apiserver status ...
I0307 18:39:14.433908    2327 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0307 18:39:14.464304    2327 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0307 18:39:14.934793    2327 api_server.go:165] Checking apiserver status ...
I0307 18:39:14.935238    2327 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0307 18:39:14.963558    2327 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0307 18:39:15.434497    2327 api_server.go:165] Checking apiserver status ...
I0307 18:39:15.434562    2327 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0307 18:39:15.451053    2327 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0307 18:39:15.934703    2327 api_server.go:165] Checking apiserver status ...
I0307 18:39:15.934761    2327 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0307 18:39:15.946368    2327 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0307 18:39:16.434362    2327 api_server.go:165] Checking apiserver status ...
I0307 18:39:16.434481    2327 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0307 18:39:16.493656    2327 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0307 18:39:16.934616    2327 api_server.go:165] Checking apiserver status ...
I0307 18:39:16.934683    2327 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0307 18:39:16.948606    2327 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0307 18:39:17.434267    2327 api_server.go:165] Checking apiserver status ...
I0307 18:39:17.434310    2327 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0307 18:39:17.445516    2327 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0307 18:39:17.934753    2327 api_server.go:165] Checking apiserver status ...
I0307 18:39:17.934802    2327 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0307 18:39:17.947241    2327 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0307 18:39:17.947256    2327 api_server.go:165] Checking apiserver status ...
I0307 18:39:17.947300    2327 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0307 18:39:17.959052    2327 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0307 18:39:17.959068    2327 kubeadm.go:608] needs reconfigure: apiserver error: timed out waiting for the condition
I0307 18:39:17.959390    2327 kubeadm.go:1120] stopping kube-system containers ...
I0307 18:39:17.959446    2327 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0307 18:39:17.996680    2327 docker.go:456] Stopping containers: [33e49e0d7cda 9860188895a5 fd753ca5c336 2ae7aa9ef0e5 9bab411d217b fe86350be8a9 394dcbc2cc2a b878e67dc224 57e4d715897c f159f72da6d3 53be5b0bbcd0 b243e3e46b2d e033fc549989 5f204ad627bb 06ce06715fc2 d8464b286bfc 5c83242d224f 5e00e9543c88 6e2adc9f3ed1 dbfb35dfbefe ccc8be563342 f30f54801f88 b3f25a32b90e 0775c302028e 2264e3966fbe aa7232b156f4 411b4b519f24 8ef0c27959bb 4b6c20f4125d debaa22a0176]
I0307 18:39:17.996750    2327 ssh_runner.go:195] Run: docker stop 33e49e0d7cda 9860188895a5 fd753ca5c336 2ae7aa9ef0e5 9bab411d217b fe86350be8a9 394dcbc2cc2a b878e67dc224 57e4d715897c f159f72da6d3 53be5b0bbcd0 b243e3e46b2d e033fc549989 5f204ad627bb 06ce06715fc2 d8464b286bfc 5c83242d224f 5e00e9543c88 6e2adc9f3ed1 dbfb35dfbefe ccc8be563342 f30f54801f88 b3f25a32b90e 0775c302028e 2264e3966fbe aa7232b156f4 411b4b519f24 8ef0c27959bb 4b6c20f4125d debaa22a0176
I0307 18:39:18.037200    2327 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I0307 18:39:18.050366    2327 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0307 18:39:18.061448    2327 kubeadm.go:155] found existing configuration files:
-rw------- 1 root root 5643 Mar  6 22:22 /etc/kubernetes/admin.conf
-rw------- 1 root root 5656 Mar  7 12:02 /etc/kubernetes/controller-manager.conf
-rw------- 1 root root 1971 Mar  6 22:23 /etc/kubernetes/kubelet.conf
-rw------- 1 root root 5600 Mar  7 12:02 /etc/kubernetes/scheduler.conf

I0307 18:39:18.061484    2327 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0307 18:39:18.072066    2327 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0307 18:39:18.081016    2327 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0307 18:39:18.092925    2327 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 1
stdout:

stderr:
I0307 18:39:18.092975    2327 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0307 18:39:18.105696    2327 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0307 18:39:18.117582    2327 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 1
stdout:

stderr:
I0307 18:39:18.117621    2327 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0307 18:39:18.128027    2327 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0307 18:39:18.140028    2327 kubeadm.go:710] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I0307 18:39:18.140038    2327 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.26.1:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I0307 18:39:18.549763    2327 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.26.1:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I0307 18:39:19.286415    2327 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.26.1:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I0307 18:39:19.487763    2327 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.26.1:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I0307 18:39:19.553454    2327 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.26.1:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I0307 18:39:19.640652    2327 api_server.go:51] waiting for apiserver process to appear ...
I0307 18:39:19.640693    2327 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0307 18:39:20.152217    2327 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0307 18:39:20.652666    2327 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0307 18:39:21.152134    2327 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0307 18:39:21.652521    2327 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0307 18:39:22.151705    2327 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0307 18:39:22.653257    2327 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0307 18:39:23.152019    2327 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0307 18:39:23.652978    2327 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0307 18:39:24.152820    2327 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0307 18:39:24.651950    2327 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0307 18:39:25.154986    2327 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0307 18:39:25.652159    2327 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0307 18:39:26.152625    2327 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0307 18:39:26.652670    2327 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0307 18:39:27.152704    2327 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0307 18:39:27.319308    2327 api_server.go:71] duration metric: took 7.6786562s to wait for apiserver process to appear ...
I0307 18:39:27.319320    2327 api_server.go:87] waiting for apiserver healthz status ...
I0307 18:39:27.319423    2327 api_server.go:252] Checking apiserver healthz at https://127.0.0.1:53895/healthz ...
I0307 18:39:27.321423    2327 api_server.go:268] stopped: https://127.0.0.1:53895/healthz: Get "https://127.0.0.1:53895/healthz": EOF
I0307 18:39:27.822218    2327 api_server.go:252] Checking apiserver healthz at https://127.0.0.1:53895/healthz ...
I0307 18:39:27.823981    2327 api_server.go:268] stopped: https://127.0.0.1:53895/healthz: Get "https://127.0.0.1:53895/healthz": EOF
I0307 18:39:28.321990    2327 api_server.go:252] Checking apiserver healthz at https://127.0.0.1:53895/healthz ...
I0307 18:39:28.323445    2327 api_server.go:268] stopped: https://127.0.0.1:53895/healthz: Get "https://127.0.0.1:53895/healthz": EOF
I0307 18:39:28.822766    2327 api_server.go:252] Checking apiserver healthz at https://127.0.0.1:53895/healthz ...
I0307 18:39:28.827965    2327 api_server.go:268] stopped: https://127.0.0.1:53895/healthz: Get "https://127.0.0.1:53895/healthz": EOF
I0307 18:39:29.322116    2327 api_server.go:252] Checking apiserver healthz at https://127.0.0.1:53895/healthz ...
I0307 18:39:29.323198    2327 api_server.go:268] stopped: https://127.0.0.1:53895/healthz: Get "https://127.0.0.1:53895/healthz": EOF
I0307 18:39:29.823376    2327 api_server.go:252] Checking apiserver healthz at https://127.0.0.1:53895/healthz ...
I0307 18:39:29.829948    2327 api_server.go:268] stopped: https://127.0.0.1:53895/healthz: Get "https://127.0.0.1:53895/healthz": EOF
I0307 18:39:30.321750    2327 api_server.go:252] Checking apiserver healthz at https://127.0.0.1:53895/healthz ...
I0307 18:39:34.308817    2327 api_server.go:278] https://127.0.0.1:53895/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0307 18:39:34.308832    2327 api_server.go:102] status: https://127.0.0.1:53895/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0307 18:39:34.322330    2327 api_server.go:252] Checking apiserver healthz at https://127.0.0.1:53895/healthz ...
I0307 18:39:34.429512    2327 api_server.go:278] https://127.0.0.1:53895/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0307 18:39:34.429532    2327 api_server.go:102] status: https://127.0.0.1:53895/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0307 18:39:34.821535    2327 api_server.go:252] Checking apiserver healthz at https://127.0.0.1:53895/healthz ...
I0307 18:39:34.828079    2327 api_server.go:278] https://127.0.0.1:53895/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0307 18:39:34.828090    2327 api_server.go:102] status: https://127.0.0.1:53895/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0307 18:39:35.322027    2327 api_server.go:252] Checking apiserver healthz at https://127.0.0.1:53895/healthz ...
I0307 18:39:35.326702    2327 api_server.go:278] https://127.0.0.1:53895/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0307 18:39:35.326714    2327 api_server.go:102] status: https://127.0.0.1:53895/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0307 18:39:35.821625    2327 api_server.go:252] Checking apiserver healthz at https://127.0.0.1:53895/healthz ...
I0307 18:39:35.841600    2327 api_server.go:278] https://127.0.0.1:53895/healthz returned 200:
ok
I0307 18:39:35.865935    2327 api_server.go:140] control plane version: v1.26.1
I0307 18:39:35.865952    2327 api_server.go:130] duration metric: took 8.5466259s to wait for apiserver health ...
I0307 18:39:35.865961    2327 cni.go:84] Creating CNI manager for ""
I0307 18:39:35.865974    2327 cni.go:157] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0307 18:39:35.874096    2327 out.go:177] 🔗  Configuring bridge CNI (Container Networking Interface) ...
I0307 18:39:35.881736    2327 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0307 18:39:35.896346    2327 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (457 bytes)
I0307 18:39:35.923021    2327 system_pods.go:43] waiting for kube-system pods to appear ...
I0307 18:39:35.949570    2327 system_pods.go:59] 8 kube-system pods found
I0307 18:39:35.949624    2327 system_pods.go:61] "coredns-787d4945fb-j4h7l" [077f640c-c8b3-4bdc-8500-dcff20b2d2d5] Running
I0307 18:39:35.949631    2327 system_pods.go:61] "etcd-minikube" [a3cad740-54c2-4244-912d-b17c8f6bd2a1] Running
I0307 18:39:35.949635    2327 system_pods.go:61] "kube-apiserver-minikube" [d883ace6-7eac-4a6e-9ae8-20838e52d7eb] Running
I0307 18:39:35.949639    2327 system_pods.go:61] "kube-controller-manager-minikube" [7fe43f7b-a020-4693-b62b-153a737f9caf] Running
I0307 18:39:35.949643    2327 system_pods.go:61] "kube-proxy-5fv9k" [a8bc55b9-7d7b-4657-809d-ab99ddb2b73d] Running
I0307 18:39:35.949647    2327 system_pods.go:61] "kube-scheduler-minikube" [06a24f94-1478-4ec2-b981-0c670400a4ec] Running
I0307 18:39:35.949652    2327 system_pods.go:61] "metrics-server-5f8fcc9bb7-rzf8c" [717850fc-e71a-450d-bd55-f251cf912aed] Running
I0307 18:39:35.949655    2327 system_pods.go:61] "storage-provisioner" [c71428ea-bf17-4886-914c-17c50b308926] Running
I0307 18:39:35.949661    2327 system_pods.go:74] duration metric: took 26.6305ms to wait for pod list to return data ...
I0307 18:39:35.949694    2327 node_conditions.go:102] verifying NodePressure condition ...
I0307 18:39:35.955669    2327 node_conditions.go:122] node storage ephemeral capacity is 263174212Ki
I0307 18:39:35.955762    2327 node_conditions.go:123] node cpu capacity is 8
I0307 18:39:35.956782    2327 node_conditions.go:105] duration metric: took 7.0287ms to run NodePressure ...
I0307 18:39:35.956872    2327 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.26.1:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I0307 18:39:36.241600    2327 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0307 18:39:36.249140    2327 ops.go:34] apiserver oom_adj: -16
I0307 18:39:36.249149    2327 kubeadm.go:637] restartCluster took 28.4426737s
I0307 18:39:36.249154    2327 kubeadm.go:403] StartCluster complete in 28.7085347s
I0307 18:39:36.249165    2327 settings.go:142] acquiring lock: {Name:mk19004591210340446308469f521c5cfa3e1599 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0307 18:39:36.249341    2327 settings.go:150] Updating kubeconfig:  /root/.kube/config
I0307 18:39:36.250014    2327 lock.go:35] WriteFile acquiring /root/.kube/config: {Name:mk72a1487fd2da23da9e8181e16f352a6105bd56 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0307 18:39:36.250580    2327 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.26.1/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0307 18:39:36.253252    2327 config.go:180] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.26.1
I0307 18:39:36.251679    2327 addons.go:489] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:true ingress-dns:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:true nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false]
I0307 18:39:36.253410    2327 addons.go:65] Setting storage-provisioner=true in profile "minikube"
I0307 18:39:36.253503    2327 addons.go:227] Setting addon storage-provisioner=true in "minikube"
W0307 18:39:36.253508    2327 addons.go:236] addon storage-provisioner should already be in state true
I0307 18:39:36.253640    2327 addons.go:65] Setting default-storageclass=true in profile "minikube"
I0307 18:39:36.253690    2327 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0307 18:39:36.253773    2327 addons.go:65] Setting ingress=true in profile "minikube"
I0307 18:39:36.253783    2327 addons.go:227] Setting addon ingress=true in "minikube"
W0307 18:39:36.253787    2327 addons.go:236] addon ingress should already be in state true
I0307 18:39:36.254536    2327 host.go:66] Checking if "minikube" exists ...
I0307 18:39:36.254551    2327 host.go:66] Checking if "minikube" exists ...
I0307 18:39:36.254845    2327 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0307 18:39:36.254974    2327 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0307 18:39:36.254977    2327 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0307 18:39:36.255043    2327 addons.go:65] Setting metrics-server=true in profile "minikube"
I0307 18:39:36.255045    2327 addons.go:65] Setting dashboard=true in profile "minikube"
I0307 18:39:36.255057    2327 addons.go:227] Setting addon dashboard=true in "minikube"
I0307 18:39:36.255057    2327 addons.go:227] Setting addon metrics-server=true in "minikube"
W0307 18:39:36.255064    2327 addons.go:236] addon dashboard should already be in state true
W0307 18:39:36.255066    2327 addons.go:236] addon metrics-server should already be in state true
I0307 18:39:36.255093    2327 host.go:66] Checking if "minikube" exists ...
I0307 18:39:36.255096    2327 host.go:66] Checking if "minikube" exists ...
I0307 18:39:36.255518    2327 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0307 18:39:36.255539    2327 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0307 18:39:36.260044    2327 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0307 18:39:36.260187    2327 start.go:223] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.26.1 ContainerRuntime:docker ControlPlane:true Worker:true}
I0307 18:39:36.272376    2327 out.go:177] 🔎  Verifying Kubernetes components...
I0307 18:39:36.281792    2327 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0307 18:39:36.377020    2327 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0307 18:39:36.385023    2327 out.go:177]     ▪ Using image registry.k8s.io/ingress-nginx/controller:v1.5.1
I0307 18:39:36.377715    2327 addons.go:227] Setting addon default-storageclass=true in "minikube"
I0307 18:39:36.384762    2327 addons.go:419] installing /etc/kubernetes/addons/storage-provisioner.yaml
W0307 18:39:36.390278    2327 addons.go:236] addon default-storageclass should already be in state true
I0307 18:39:36.394868    2327 out.go:177]     ▪ Using image registry.k8s.io/metrics-server/metrics-server:v0.6.2
I0307 18:39:36.394901    2327 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0307 18:39:36.400285    2327 out.go:177]     ▪ Using image docker.io/kubernetesui/dashboard:v2.7.0
I0307 18:39:36.400346    2327 host.go:66] Checking if "minikube" exists ...
I0307 18:39:36.409917    2327 addons.go:419] installing /etc/kubernetes/addons/metrics-apiservice.yaml
I0307 18:39:36.409942    2327 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/metrics-apiservice.yaml (424 bytes)
I0307 18:39:36.410052    2327 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0307 18:39:36.405024    2327 out.go:177]     ▪ Using image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20220916-gd32f8c343
I0307 18:39:36.405067    2327 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0307 18:39:36.405272    2327 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0307 18:39:36.439170    2327 out.go:177]     ▪ Using image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20220916-gd32f8c343
I0307 18:39:36.447413    2327 out.go:177]     ▪ Using image docker.io/kubernetesui/metrics-scraper:v1.0.8
I0307 18:39:36.454770    2327 addons.go:419] installing /etc/kubernetes/addons/dashboard-ns.yaml
I0307 18:39:36.454793    2327 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-ns.yaml (759 bytes)
I0307 18:39:36.454871    2327 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0307 18:39:36.447935    2327 addons.go:419] installing /etc/kubernetes/addons/ingress-deploy.yaml
I0307 18:39:36.454946    2327 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/ingress-deploy.yaml (16105 bytes)
I0307 18:39:36.455018    2327 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0307 18:39:36.496403    2327 addons.go:419] installing /etc/kubernetes/addons/storageclass.yaml
I0307 18:39:36.496421    2327 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0307 18:39:36.496480    2327 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0307 18:39:36.517276    2327 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53896 SSHKeyPath:/root/.minikube/machines/minikube/id_rsa Username:docker}
I0307 18:39:36.524941    2327 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53896 SSHKeyPath:/root/.minikube/machines/minikube/id_rsa Username:docker}
I0307 18:39:36.551014    2327 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53896 SSHKeyPath:/root/.minikube/machines/minikube/id_rsa Username:docker}
I0307 18:39:36.551059    2327 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53896 SSHKeyPath:/root/.minikube/machines/minikube/id_rsa Username:docker}
I0307 18:39:36.570456    2327 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53896 SSHKeyPath:/root/.minikube/machines/minikube/id_rsa Username:docker}
I0307 18:39:36.661799    2327 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.26.1/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0307 18:39:36.674517    2327 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.26.1/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml
I0307 18:39:36.727937    2327 addons.go:419] installing /etc/kubernetes/addons/metrics-server-deployment.yaml
I0307 18:39:36.727950    2327 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/metrics-server-deployment.yaml (1907 bytes)
I0307 18:39:36.734066    2327 addons.go:419] installing /etc/kubernetes/addons/dashboard-clusterrole.yaml
I0307 18:39:36.734081    2327 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrole.yaml (1001 bytes)
I0307 18:39:36.735506    2327 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.26.1/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0307 18:39:36.820004    2327 addons.go:419] installing /etc/kubernetes/addons/metrics-server-rbac.yaml
I0307 18:39:36.820051    2327 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/metrics-server-rbac.yaml (2175 bytes)
I0307 18:39:36.843024    2327 addons.go:419] installing /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml
I0307 18:39:36.843076    2327 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml (1018 bytes)
I0307 18:39:36.945147    2327 addons.go:419] installing /etc/kubernetes/addons/metrics-server-service.yaml
I0307 18:39:36.945161    2327 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/metrics-server-service.yaml (446 bytes)
I0307 18:39:37.022015    2327 addons.go:419] installing /etc/kubernetes/addons/dashboard-configmap.yaml
I0307 18:39:37.022069    2327 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-configmap.yaml (837 bytes)
I0307 18:39:37.327599    2327 addons.go:419] installing /etc/kubernetes/addons/dashboard-dp.yaml
I0307 18:39:37.327614    2327 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-dp.yaml (4288 bytes)
I0307 18:39:37.332920    2327 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.26.1/kubectl apply -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml
I0307 18:39:37.620890    2327 addons.go:419] installing /etc/kubernetes/addons/dashboard-role.yaml
I0307 18:39:37.620940    2327 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-role.yaml (1724 bytes)
I0307 18:39:38.036803    2327 addons.go:419] installing /etc/kubernetes/addons/dashboard-rolebinding.yaml
I0307 18:39:38.036842    2327 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-rolebinding.yaml (1046 bytes)
I0307 18:39:38.735794    2327 addons.go:419] installing /etc/kubernetes/addons/dashboard-sa.yaml
I0307 18:39:38.735855    2327 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-sa.yaml (837 bytes)
I0307 18:39:39.241675    2327 addons.go:419] installing /etc/kubernetes/addons/dashboard-secret.yaml
I0307 18:39:39.241695    2327 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-secret.yaml (1389 bytes)
I0307 18:39:39.346510    2327 addons.go:419] installing /etc/kubernetes/addons/dashboard-svc.yaml
I0307 18:39:39.346525    2327 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-svc.yaml (1294 bytes)
I0307 18:39:39.618675    2327 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.26.1/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I0307 18:39:45.340785    2327 ssh_runner.go:235] Completed: sudo systemctl is-active --quiet service kubelet: (9.0596639s)
I0307 18:39:45.340955    2327 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0307 18:39:45.341245    2327 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.26.1/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml": (9.0913519s)
I0307 18:39:45.343503    2327 start.go:892] CoreDNS already contains "host.minikube.internal" host record, skipping...
I0307 18:39:45.692536    2327 api_server.go:51] waiting for apiserver process to appear ...
I0307 18:39:45.692571    2327 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0307 18:40:03.323395    2327 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.26.1/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (26.6622579s)
I0307 18:40:11.416055    2327 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.26.1/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (34.6812136s)
I0307 18:40:11.416128    2327 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.26.1/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml: (34.7422963s)
I0307 18:40:11.416146    2327 addons.go:457] Verifying addon ingress=true in "minikube"
I0307 18:40:11.416959    2327 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.26.1/kubectl apply -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml: (34.0847122s)
I0307 18:40:11.446811    2327 out.go:177] 🔎  Verifying ingress addon...
I0307 18:40:11.446878    2327 addons.go:457] Verifying addon metrics-server=true in "minikube"
I0307 18:40:11.417268    2327 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.26.1/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: (31.7992469s)
I0307 18:40:11.417517    2327 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (25.7247301s)
I0307 18:40:11.463518    2327 out.go:177] 💡  Some dashboard features require the metrics-server addon. To enable all features please run:

	minikube addons enable metrics-server	


I0307 18:40:11.456512    2327 api_server.go:71] duration metric: took 35.1969793s to wait for apiserver process to appear ...
I0307 18:40:11.457586    2327 kapi.go:75] Waiting for pod with label "app.kubernetes.io/name=ingress-nginx" in ns "ingress-nginx" ...
I0307 18:40:11.470249    2327 api_server.go:87] waiting for apiserver healthz status ...
I0307 18:40:11.470265    2327 api_server.go:252] Checking apiserver healthz at https://127.0.0.1:53895/healthz ...
I0307 18:40:11.543204    2327 kapi.go:86] Found 3 Pods for label selector app.kubernetes.io/name=ingress-nginx
I0307 18:40:11.543224    2327 kapi.go:107] duration metric: took 85.6427ms to wait for app.kubernetes.io/name=ingress-nginx ...
I0307 18:40:11.553541    2327 out.go:177] 🌟  Enabled addons: storage-provisioner, default-storageclass, metrics-server, dashboard, ingress
I0307 18:40:11.561640    2327 addons.go:492] enable addons completed in 35.3108185s: enabled=[storage-provisioner default-storageclass metrics-server dashboard ingress]
I0307 18:40:11.616718    2327 api_server.go:278] https://127.0.0.1:53895/healthz returned 200:
ok
I0307 18:40:11.618433    2327 api_server.go:140] control plane version: v1.26.1
I0307 18:40:11.618449    2327 api_server.go:130] duration metric: took 148.1906ms to wait for apiserver health ...
I0307 18:40:11.618458    2327 system_pods.go:43] waiting for kube-system pods to appear ...
I0307 18:40:11.640409    2327 system_pods.go:59] 8 kube-system pods found
I0307 18:40:11.640437    2327 system_pods.go:61] "coredns-787d4945fb-j4h7l" [077f640c-c8b3-4bdc-8500-dcff20b2d2d5] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0307 18:40:11.640442    2327 system_pods.go:61] "etcd-minikube" [a3cad740-54c2-4244-912d-b17c8f6bd2a1] Running
I0307 18:40:11.640448    2327 system_pods.go:61] "kube-apiserver-minikube" [d883ace6-7eac-4a6e-9ae8-20838e52d7eb] Running
I0307 18:40:11.640454    2327 system_pods.go:61] "kube-controller-manager-minikube" [7fe43f7b-a020-4693-b62b-153a737f9caf] Running
I0307 18:40:11.640460    2327 system_pods.go:61] "kube-proxy-5fv9k" [a8bc55b9-7d7b-4657-809d-ab99ddb2b73d] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0307 18:40:11.640465    2327 system_pods.go:61] "kube-scheduler-minikube" [06a24f94-1478-4ec2-b981-0c670400a4ec] Running
I0307 18:40:11.640472    2327 system_pods.go:61] "metrics-server-5f8fcc9bb7-rzf8c" [717850fc-e71a-450d-bd55-f251cf912aed] Running / Ready:ContainersNotReady (containers with unready status: [metrics-server]) / ContainersReady:ContainersNotReady (containers with unready status: [metrics-server])
I0307 18:40:11.640478    2327 system_pods.go:61] "storage-provisioner" [c71428ea-bf17-4886-914c-17c50b308926] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0307 18:40:11.640484    2327 system_pods.go:74] duration metric: took 22.0209ms to wait for pod list to return data ...
I0307 18:40:11.640495    2327 kubeadm.go:578] duration metric: took 35.3809754s to wait for : map[apiserver:true system_pods:true] ...
I0307 18:40:11.640507    2327 node_conditions.go:102] verifying NodePressure condition ...
I0307 18:40:11.723426    2327 node_conditions.go:122] node storage ephemeral capacity is 263174212Ki
I0307 18:40:11.723867    2327 node_conditions.go:123] node cpu capacity is 8
I0307 18:40:11.723893    2327 node_conditions.go:105] duration metric: took 83.3789ms to run NodePressure ...
I0307 18:40:11.723929    2327 start.go:228] waiting for startup goroutines ...
I0307 18:40:11.723942    2327 start.go:233] waiting for cluster config update ...
I0307 18:40:11.724073    2327 start.go:240] writing updated cluster config ...
I0307 18:40:11.724594    2327 ssh_runner.go:195] Run: rm -f paused
I0307 18:40:12.264485    2327 start.go:555] kubectl: 1.26.2, cluster: 1.26.1 (minor skew: 0)
I0307 18:40:12.269870    2327 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* -- Logs begin at Tue 2023-03-07 21:38:50 UTC, end at Wed 2023-03-08 01:50:32 UTC. --
Mar 07 22:40:54 minikube dockerd[879]: time="2023-03-07T22:40:54.337166900Z" level=info msg="ignoring event" container=151ba374493a5bacd978a863df4d76727144b92d957c1ef18383cb52375f035c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 07 22:40:56 minikube dockerd[879]: time="2023-03-07T22:40:56.046637800Z" level=info msg="ignoring event" container=b4a0eb5740867f6199ff6953098a1bee7afd76583375bac100eb646e1e3e704e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 07 22:40:58 minikube dockerd[879]: time="2023-03-07T22:40:58.715238500Z" level=error msg="Failed to compute size of container rootfs 4df88ee9297cd9fc84569927631253aca702810e986679c96e0d0cdcd9489c31: mount does not exist"
Mar 07 22:41:38 minikube dockerd[879]: time="2023-03-07T22:41:38.141934200Z" level=info msg="ignoring event" container=7c0940afedb9340474d7249135908cd2dc471c6b819b3b13a19d6c073137ab69 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 07 22:41:38 minikube dockerd[879]: time="2023-03-07T22:41:38.877595900Z" level=info msg="ignoring event" container=25580ad80da7e1b0658bfd9597fb44ac112f1ba4aba3ec02de158413636d45d3 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 07 22:42:38 minikube dockerd[879]: time="2023-03-07T22:42:38.801453400Z" level=info msg="ignoring event" container=b0b03d63affefb70b3522629b222253b0439b4be9fbeac6891f5e1d845fa1658 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 07 22:44:15 minikube dockerd[879]: time="2023-03-07T22:44:15.646660100Z" level=info msg="ignoring event" container=f95ad635a97c0c6190ea4da260facc4d191dcaadd5fb0b9e58e429ff63285098 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 07 22:44:16 minikube dockerd[879]: time="2023-03-07T22:44:16.243509300Z" level=info msg="ignoring event" container=a62cd896c9a3f3aeb3cd1bda7db47fddfc9e5ef06960404726576c80df266bf2 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 07 22:44:18 minikube dockerd[879]: time="2023-03-07T22:44:18.445220700Z" level=info msg="ignoring event" container=37abba3d9caa8f81db623bd2b4fc70ad2c1a9e01ca4bfb03d7d242a2ef809f2c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 07 22:44:19 minikube dockerd[879]: time="2023-03-07T22:44:19.946172200Z" level=info msg="ignoring event" container=dc9dc878770a91926c595b407e81a4019d6a495e50bb9cbfa24ea5ab3c150bb8 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 07 22:44:27 minikube dockerd[879]: time="2023-03-07T22:44:27.043284800Z" level=info msg="ignoring event" container=d13bb2d5f4128fa4c399d0a21faeecc70d412b76b79e51c9450765eefdbda470 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 07 22:44:29 minikube dockerd[879]: time="2023-03-07T22:44:29.037336700Z" level=info msg="ignoring event" container=1ccc318e2a57eb7aa9926e5aeae0be78884e416fc06311c9e9da2e1aa099fee4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 07 22:44:34 minikube dockerd[879]: time="2023-03-07T22:44:34.039057000Z" level=info msg="ignoring event" container=5ce4a6804f3dd8938a1b66eefc449a342e159fa49b4a7cd92adfa821c2d8a1c8 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 07 22:44:34 minikube dockerd[879]: time="2023-03-07T22:44:34.849368200Z" level=info msg="ignoring event" container=3f0a291940f22f5e9558c9bb25fe125703447f5033d7275076cdb415582b3d85 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 08 00:05:43 minikube dockerd[879]: time="2023-03-08T00:05:43.943072800Z" level=warning msg="reference for unknown type: " digest="sha256:15be4666c53052484dd2992efacf2f50ea77a78ae8aa21ccd91af6baaa7ea22f" remote="registry.k8s.io/ingress-nginx/controller@sha256:15be4666c53052484dd2992efacf2f50ea77a78ae8aa21ccd91af6baaa7ea22f"
Mar 08 00:08:40 minikube dockerd[879]: time="2023-03-08T00:08:40.237669400Z" level=info msg="Container failed to exit within 1s of signal 15 - using the force" container=82c6bba8920781e340501bd4d5ba10110c92cea79d0603864c1b5e0b5d472f7e
Mar 08 00:08:42 minikube dockerd[879]: time="2023-03-08T00:08:42.115773000Z" level=info msg="ignoring event" container=82c6bba8920781e340501bd4d5ba10110c92cea79d0603864c1b5e0b5d472f7e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 08 00:08:43 minikube dockerd[879]: time="2023-03-08T00:08:43.481805000Z" level=info msg="ignoring event" container=50f1a4df7dfbfe5814188b4c2f56a97ab3b4899a75d7d7e1d138b376ee06c069 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 08 01:23:40 minikube dockerd[879]: time="2023-03-08T01:23:40.632652400Z" level=info msg="ignoring event" container=02eccfb690b61285e21ccb8747ca4b41fc34b9d557973c591baae9f20be50fbb module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 08 01:23:42 minikube dockerd[879]: time="2023-03-08T01:23:42.531195000Z" level=info msg="ignoring event" container=e99526e17e5ee896a786892d69572358136a2659b7393cd5ddd51a7233b38b4c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 08 01:25:06 minikube dockerd[879]: time="2023-03-08T01:25:06.649981400Z" level=info msg="Container failed to exit within 1s of signal 15 - using the force" container=cfa2a1da6603116ae1fb7a2ef07baebcc0914e0b779cbcb741d73e2b3e50d02b
Mar 08 01:25:06 minikube dockerd[879]: time="2023-03-08T01:25:06.891583700Z" level=info msg="ignoring event" container=cfa2a1da6603116ae1fb7a2ef07baebcc0914e0b779cbcb741d73e2b3e50d02b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 08 01:25:08 minikube dockerd[879]: time="2023-03-08T01:25:08.838803900Z" level=info msg="ignoring event" container=1ebdbaf4c8e749bd8f9550542d8131aa338b03fe0b59f5bf1d5f597573443edf module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 08 01:27:23 minikube dockerd[879]: time="2023-03-08T01:27:23.549430700Z" level=info msg="ignoring event" container=ccbb58709e3cacb4d4fd50b8c648489d2655a9d879dae7617b062c35edb14acb module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 08 01:27:24 minikube dockerd[879]: time="2023-03-08T01:27:24.026797400Z" level=info msg="ignoring event" container=a80b60be4ffdb49e973b9b5030b4fb59d53dfd9277e21c477843057510d9a207 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 08 01:33:15 minikube dockerd[879]: time="2023-03-08T01:33:15.158306200Z" level=info msg="ignoring event" container=076c878b95ababf586bed350b0887d1f95ab905464a60e29a7adea72af38e619 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 08 01:33:15 minikube dockerd[879]: time="2023-03-08T01:33:15.254373300Z" level=info msg="ignoring event" container=ddd71499a53c1f9785bea5544d2670355d7a979d002c51f59f0cf481680b1c44 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 08 01:33:16 minikube dockerd[879]: time="2023-03-08T01:33:16.414370600Z" level=info msg="ignoring event" container=c9743e2650a7ceb50293b8e4d7d1e35872b41f078a488154a2696cf2babffa9c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 08 01:33:17 minikube dockerd[879]: time="2023-03-08T01:33:17.619719900Z" level=info msg="ignoring event" container=4ec5d1dc14dd013ef273bb2febb99fad943c1db85216e12ba076d578912b1868 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 08 01:33:18 minikube dockerd[879]: time="2023-03-08T01:33:18.552177800Z" level=info msg="ignoring event" container=4de4bd9a770aff83a0873afbbcff7f9804780bae1687ef20a86c0f2a3f5b7268 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 08 01:34:11 minikube dockerd[879]: time="2023-03-08T01:34:11.821104800Z" level=info msg="ignoring event" container=7b13b66bad2526248b29298c1f1b915af4351f04cc9b939e31f654a400d82e62 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 08 01:34:12 minikube dockerd[879]: time="2023-03-08T01:34:12.459922700Z" level=info msg="ignoring event" container=685c714234973c46e4133c1e0c9a2fb43dbac838e20e87be639cf1e7205cee5f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 08 01:36:11 minikube dockerd[879]: time="2023-03-08T01:36:11.183319000Z" level=info msg="Container failed to exit within 1s of signal 15 - using the force" container=9ad987dd4e3c15f330bd953f0cf47fee13e1695a5f6f0a336eb6220efcff4822
Mar 08 01:36:11 minikube dockerd[879]: time="2023-03-08T01:36:11.394515600Z" level=info msg="ignoring event" container=9ad987dd4e3c15f330bd953f0cf47fee13e1695a5f6f0a336eb6220efcff4822 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 08 01:36:12 minikube dockerd[879]: time="2023-03-08T01:36:12.821154600Z" level=info msg="ignoring event" container=60aac5bac34b73bb0e5f67b59f2526c2c1762e06d01e723db4e313a63e122541 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 08 01:36:24 minikube dockerd[879]: time="2023-03-08T01:36:24.815126400Z" level=info msg="ignoring event" container=ed98dde9dc55afe422cb86ebb817ed44e27de203b6efaa2cad5621c6124ba507 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 08 01:36:24 minikube dockerd[879]: time="2023-03-08T01:36:24.915361600Z" level=info msg="ignoring event" container=83d9b02a68058964940340cde75c930e18488a3d7d256d37d6ed58ccd95141a4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 08 01:36:25 minikube dockerd[879]: time="2023-03-08T01:36:25.543193000Z" level=info msg="ignoring event" container=a005c2e5d9407a46b68be7f9dc4e438c508ac608c24aec338ad91da5f63c17cd module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 08 01:36:28 minikube dockerd[879]: time="2023-03-08T01:36:28.631813000Z" level=info msg="ignoring event" container=0bb6efcb9d00400254474ab285685c381898f9b71238bd7f56b6f0cb1b389771 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 08 01:36:29 minikube dockerd[879]: time="2023-03-08T01:36:29.415337500Z" level=info msg="ignoring event" container=751bc35ca1cad51f63a07b888c3def0425a0f2363dc17bc3a81bea5a90db5f24 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 08 01:37:55 minikube dockerd[879]: time="2023-03-08T01:37:55.443273400Z" level=info msg="Container failed to exit within 1s of signal 15 - using the force" container=27198782b6dfaf3dd63cfe5baab67f0bdfe07bb40921f3aafa9e3883e78f1151
Mar 08 01:37:55 minikube dockerd[879]: time="2023-03-08T01:37:55.659560500Z" level=info msg="ignoring event" container=27198782b6dfaf3dd63cfe5baab67f0bdfe07bb40921f3aafa9e3883e78f1151 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 08 01:37:56 minikube dockerd[879]: time="2023-03-08T01:37:56.896253800Z" level=info msg="ignoring event" container=6a7fb92e68ac702f541ba23ddc98067361cb0fc9be5e2787e32f0be8d4ddd939 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 08 01:38:18 minikube dockerd[879]: time="2023-03-08T01:38:18.622717200Z" level=info msg="ignoring event" container=ef56e65d737c77b2dedc988011ecbd3611a134552537ddeffaef7c899c891e33 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 08 01:38:18 minikube dockerd[879]: time="2023-03-08T01:38:18.638842800Z" level=info msg="ignoring event" container=fe30ca798e81425cf3dd8bdc3e7e19e9ba8a45a1e727ca501e436a4d8fb8c960 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 08 01:38:20 minikube dockerd[879]: time="2023-03-08T01:38:20.620299600Z" level=info msg="ignoring event" container=c4c971cc9ddb25c509186d0ab7d3807ef10269cd3c3bcfadc5b817806b4c3515 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 08 01:38:22 minikube dockerd[879]: time="2023-03-08T01:38:22.016339400Z" level=info msg="ignoring event" container=c48b0c60c12ebec4c01265f3d42f63b3e7bcda6fd8ad548b75e279e360f20bc4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 08 01:38:23 minikube dockerd[879]: time="2023-03-08T01:38:23.043750500Z" level=info msg="ignoring event" container=926d433948c91950ee117c345d798a0c832d36a384af49fa5af337de35546ca0 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 08 01:43:18 minikube dockerd[879]: time="2023-03-08T01:43:18.532015700Z" level=info msg="Container failed to exit within 1s of signal 15 - using the force" container=b12aafe08306ba38b4dc5598d98f597327a0e0bd16a31fd165bebd209fc96b73
Mar 08 01:43:18 minikube dockerd[879]: time="2023-03-08T01:43:18.928046100Z" level=info msg="ignoring event" container=b12aafe08306ba38b4dc5598d98f597327a0e0bd16a31fd165bebd209fc96b73 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 08 01:43:20 minikube dockerd[879]: time="2023-03-08T01:43:20.579218700Z" level=info msg="ignoring event" container=762b3663d4f7b67e8b0e23493984bff5baacf31042c02195bb89f3c42f9eb31e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 08 01:43:30 minikube dockerd[879]: time="2023-03-08T01:43:30.521526800Z" level=info msg="ignoring event" container=c926fa1a5973614212971adc384406bc9b37597618c6ff3d445f2139fa5ca1bc module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 08 01:43:31 minikube dockerd[879]: time="2023-03-08T01:43:31.069193700Z" level=info msg="ignoring event" container=8b0f17f68e41eb6fdd95f92cc786b4b46317556cb391e13c5efddb826d4172e2 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 08 01:45:54 minikube dockerd[879]: time="2023-03-08T01:45:54.320725700Z" level=info msg="ignoring event" container=62013df2944eb06a5ab1156e569a03c23882e9bc831f3dfafeeec1f0fb2c7231 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 08 01:45:54 minikube dockerd[879]: time="2023-03-08T01:45:54.415543100Z" level=info msg="ignoring event" container=346f91febcecbb1bbd2999bf45b39cdd0b2bdf28fbce1f5af7d51dd64bc42b7c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 08 01:45:56 minikube dockerd[879]: time="2023-03-08T01:45:56.545712900Z" level=info msg="ignoring event" container=bc87d6511c82005e1617b52c4cad11235ba2c3a2e39515194bfbc3674274c727 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 08 01:45:56 minikube dockerd[879]: time="2023-03-08T01:45:56.702992500Z" level=info msg="ignoring event" container=5f82a85d1589c1ab744e12dc3c3b7b1502a91a488249792af3f5e8f32e8423f2 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 08 01:45:58 minikube dockerd[879]: time="2023-03-08T01:45:58.954790200Z" level=info msg="ignoring event" container=8d3053dbe1282a560be8fe6c7880a1980e208cd4f17d4c17d3be7a97a4679e55 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 08 01:46:23 minikube dockerd[879]: time="2023-03-08T01:46:23.506798300Z" level=info msg="ignoring event" container=4e66457ad3d76f833c9e34b5e94836e3ec21ac9701084468e79af74a12393425 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 08 01:46:24 minikube dockerd[879]: time="2023-03-08T01:46:24.143063200Z" level=info msg="ignoring event" container=d269c38b3b3e413ba220fc911bd86764a6d0a868f723775caabf98b1f3547f55 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"

* 
* ==> container status <==
* CONTAINER           IMAGE                                                                                                CREATED             STATE               NAME                        ATTEMPT             POD ID
e519b653bab51       f2e1146a6d96a                                                                                        4 minutes ago       Running             controller                  0                   90c1b7d583748
bc87d6511c820       520347519a8ca                                                                                        4 minutes ago       Exited              patch                       1                   8d3053dbe1282
62013df2944eb       520347519a8ca                                                                                        4 minutes ago       Exited              create                      0                   5f82a85d1589c
ce623f4491d20       stephengrider/multi-server@sha256:276e6416dba68f90bd1dc0d54fff19b3ac8d49ca8b4851dc9a2fa852a4092054   3 hours ago         Running             server                      0                   14ff0f592c3d1
9b056f6e67ddd       stephengrider/multi-server@sha256:276e6416dba68f90bd1dc0d54fff19b3ac8d49ca8b4851dc9a2fa852a4092054   3 hours ago         Running             server                      0                   801d97231302c
78371673bd2fc       stephengrider/multi-server@sha256:276e6416dba68f90bd1dc0d54fff19b3ac8d49ca8b4851dc9a2fa852a4092054   3 hours ago         Running             server                      0                   997459e16e433
d232adc70d8c3       stephengrider/multi-worker@sha256:5fbab5f86e6a4d499926349a5f0ec032c42e7f7450acc98b053791df26dc4d2b   3 hours ago         Running             worker                      0                   34c533afcebc1
f749887670078       postgres@sha256:50a96a21f2992518c2cb4601467cf27c7ac852542d8913c1872fe45cd6449947                     3 hours ago         Running             postgres                    1                   75a1e8f338855
b0b03d63affef       postgres@sha256:50a96a21f2992518c2cb4601467cf27c7ac852542d8913c1872fe45cd6449947                     3 hours ago         Exited              postgres                    0                   75a1e8f338855
31f8a410f4031       25561daa66605                                                                                        4 hours ago         Running             metrics-server              4                   fac54210f25f3
4a8313a5b5cd5       07655ddf2eebe                                                                                        4 hours ago         Running             kubernetes-dashboard        4                   cb5c5c32de6ba
2c19c781076c6       6e38f40d628db                                                                                        4 hours ago         Running             storage-provisioner         9                   b816a686048bc
fa8ce92c282ba       stephengrider/multi-client@sha256:855452509d6d9f13dbe1cd34fa3a21d7f6e7d1f0fafb38d1e715dda8e3d17f46   4 hours ago         Running             client                      2                   f127884c8cdf7
6432430427c7f       stephengrider/multi-client@sha256:855452509d6d9f13dbe1cd34fa3a21d7f6e7d1f0fafb38d1e715dda8e3d17f46   4 hours ago         Running             client                      2                   4e460310891b3
015aaa5d65fc8       stephengrider/multi-client@sha256:855452509d6d9f13dbe1cd34fa3a21d7f6e7d1f0fafb38d1e715dda8e3d17f46   4 hours ago         Running             client                      2                   e169782ef6633
66dd287e85747       115053965e86b                                                                                        4 hours ago         Running             dashboard-metrics-scraper   2                   1d8592a345097
15d8a9ba0ca2b       5185b96f0becf                                                                                        4 hours ago         Running             coredns                     5                   9b17fb11a5ba2
5ebb07376bc48       redis@sha256:e50c7e23f79ae81351beacb20e004720d4bed657415e68c2b1a2b5557c075ce0                        4 hours ago         Running             redis                       2                   86e3a7bb238d8
603a9beb1b0aa       25561daa66605                                                                                        4 hours ago         Exited              metrics-server              3                   fac54210f25f3
f20c1d258a4d7       07655ddf2eebe                                                                                        4 hours ago         Exited              kubernetes-dashboard        3                   cb5c5c32de6ba
c58103e28ec7d       6e38f40d628db                                                                                        4 hours ago         Exited              storage-provisioner         8                   b816a686048bc
a60fa573a6bda       46a6bb3c77ce0                                                                                        4 hours ago         Running             kube-proxy                  6                   5b13208f0bb62
6ff83d33cdacd       deb04688c4a35                                                                                        4 hours ago         Running             kube-apiserver              6                   8402c749e0461
137bddb8edb6b       e9c08e11b07f6                                                                                        4 hours ago         Running             kube-controller-manager     6                   cb45b6afcce1a
0655a87664f05       655493523f607                                                                                        4 hours ago         Running             kube-scheduler              6                   1d91419f0ee8d
39c5e37b7e113       fce326961ae2d                                                                                        4 hours ago         Running             etcd                        6                   14845608a5520
eb369d718df7b       stephengrider/multi-client@sha256:855452509d6d9f13dbe1cd34fa3a21d7f6e7d1f0fafb38d1e715dda8e3d17f46   14 hours ago        Exited              client                      1                   2751afaa7c22d
8a563980354bd       redis@sha256:e50c7e23f79ae81351beacb20e004720d4bed657415e68c2b1a2b5557c075ce0                        14 hours ago        Exited              redis                       1                   ff35153847c3b
e87050e3afe80       stephengrider/multi-client@sha256:855452509d6d9f13dbe1cd34fa3a21d7f6e7d1f0fafb38d1e715dda8e3d17f46   14 hours ago        Exited              client                      1                   ee2e3f462133c
701030eef6959       stephengrider/multi-client@sha256:855452509d6d9f13dbe1cd34fa3a21d7f6e7d1f0fafb38d1e715dda8e3d17f46   14 hours ago        Exited              client                      1                   1531b0006bbdd
fd753ca5c336c       5185b96f0becf                                                                                        14 hours ago        Exited              coredns                     4                   b878e67dc2245
410237a5d5185       115053965e86b                                                                                        14 hours ago        Exited              dashboard-metrics-scraper   1                   64998627b7c0b
9bab411d217ba       46a6bb3c77ce0                                                                                        14 hours ago        Exited              kube-proxy                  5                   57e4d715897c9
53be5b0bbcd07       e9c08e11b07f6                                                                                        14 hours ago        Exited              kube-controller-manager     5                   06ce06715fc21
b243e3e46b2d3       655493523f607                                                                                        14 hours ago        Exited              kube-scheduler              5                   d8464b286bfce
e033fc5499895       deb04688c4a35                                                                                        14 hours ago        Exited              kube-apiserver              5                   5c83242d224f1
5f204ad627bb9       fce326961ae2d                                                                                        14 hours ago        Exited              etcd                        5                   5e00e9543c881

* 
* ==> coredns [15d8a9ba0ca2] <==
* [INFO] 10.244.0.43:60416 - 8880 "AAAA IN registry.npmjs.org.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000121s
[INFO] 10.244.0.43:60416 - 8580 "A IN registry.npmjs.org.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.0000491s
[INFO] 10.244.0.43:49871 - 45670 "AAAA IN registry.npmjs.org. udp 36 false 512" NOERROR qr,aa,tc,rd,ra 1140 0.0004572s
[INFO] 10.244.0.43:49871 - 45370 "A IN registry.npmjs.org. udp 36 false 512" NOERROR qr,aa,rd,ra 444 0.000122s
[INFO] 10.244.0.42:57516 - 22300 "AAAA IN registry.npmjs.org.default.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.0002978s
[INFO] 10.244.0.42:57516 - 21700 "A IN registry.npmjs.org.default.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.0004719s
[INFO] 10.244.0.42:54515 - 53570 "AAAA IN registry.npmjs.org.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.0001639s
[INFO] 10.244.0.42:54515 - 52970 "A IN registry.npmjs.org.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.0001185s
[INFO] 10.244.0.42:47334 - 28250 "A IN registry.npmjs.org.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.0002442s
[INFO] 10.244.0.42:47334 - 28750 "AAAA IN registry.npmjs.org.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.0003092s
[INFO] 10.244.0.42:41846 - 11735 "AAAA IN registry.npmjs.org. udp 36 false 512" NOERROR qr,tc,rd,ra 1140 0.0386102s
[INFO] 10.244.0.42:41846 - 11235 "A IN registry.npmjs.org. udp 36 false 512" NOERROR qr,rd,ra 444 0.0384451s
[INFO] 10.244.0.46:60164 - 18320 "AAAA IN registry.npmjs.org.default.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.0002476s
[INFO] 10.244.0.46:60164 - 18020 "A IN registry.npmjs.org.default.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.0004082s
[INFO] 10.244.0.46:39486 - 65335 "AAAA IN registry.npmjs.org.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.0001575s
[INFO] 10.244.0.46:39486 - 65035 "A IN registry.npmjs.org.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.0002265s
[INFO] 10.244.0.46:59696 - 18585 "AAAA IN registry.npmjs.org.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.0001348s
[INFO] 10.244.0.46:59696 - 18285 "A IN registry.npmjs.org.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.0002317s
[INFO] 10.244.0.46:48628 - 46540 "AAAA IN registry.npmjs.org. udp 36 false 512" NOERROR qr,aa,tc,rd,ra 1140 0.0003099s
[INFO] 10.244.0.46:48628 - 46240 "A IN registry.npmjs.org. udp 36 false 512" NOERROR qr,aa,rd,ra 444 0.0005219s
[INFO] 10.244.0.53:35289 - 9255 "A IN registry.npmjs.org.default.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.0004446s
[INFO] 10.244.0.53:35289 - 9755 "AAAA IN registry.npmjs.org.default.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.0006234s
[INFO] 10.244.0.53:44258 - 35825 "AAAA IN registry.npmjs.org.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.0001829s
[INFO] 10.244.0.53:44258 - 35425 "A IN registry.npmjs.org.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.0002374s
[INFO] 10.244.0.53:50430 - 48840 "AAAA IN registry.npmjs.org.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.0002215s
[INFO] 10.244.0.53:50430 - 48440 "A IN registry.npmjs.org.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.0003018s
[INFO] 10.244.0.53:45452 - 28790 "AAAA IN registry.npmjs.org. udp 36 false 512" NOERROR qr,tc,rd,ra 1140 0.0300299s
[INFO] 10.244.0.53:45452 - 28490 "A IN registry.npmjs.org. udp 36 false 512" NOERROR qr,rd,ra 444 0.0306312s
[INFO] 10.244.0.53:42537 - 64235 "A IN redis-cluster-service.default.svc.cluster.local. udp 65 false 512" NOERROR qr,aa,rd 128 0.0004402s
[INFO] 10.244.0.53:37914 - 7204 "A IN redis-cluster-service.default.svc.cluster.local. udp 65 false 512" NOERROR qr,aa,rd 128 0.0002264s
[INFO] 10.244.0.54:51570 - 30060 "AAAA IN registry.npmjs.org.default.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.0002979s
[INFO] 10.244.0.54:51570 - 29760 "A IN registry.npmjs.org.default.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.0005241s
[INFO] 10.244.0.54:46234 - 64770 "AAAA IN registry.npmjs.org.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.0001142s
[INFO] 10.244.0.54:46234 - 64570 "A IN registry.npmjs.org.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.0001125s
[INFO] 10.244.0.54:46997 - 27635 "AAAA IN registry.npmjs.org.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.0002597s
[INFO] 10.244.0.54:46997 - 27335 "A IN registry.npmjs.org.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.0003707s
[INFO] 10.244.0.54:58254 - 61545 "AAAA IN registry.npmjs.org. udp 36 false 512" NOERROR qr,aa,tc,rd,ra 1140 0.0003027s
[INFO] 10.244.0.54:58254 - 61345 "A IN registry.npmjs.org. udp 36 false 512" NOERROR qr,aa,rd,ra 444 0.0003781s
[INFO] 10.244.0.54:36684 - 6045 "A IN redis-cluster-service.default.svc.cluster.local. udp 65 false 512" NOERROR qr,aa,rd 128 0.0001807s
[INFO] 10.244.0.54:52233 - 25045 "A IN redis-cluster-service.default.svc.cluster.local. udp 65 false 512" NOERROR qr,aa,rd 128 0.0001563s
[INFO] 10.244.0.55:34919 - 10245 "AAAA IN registry.npmjs.org.default.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.0007238s
[INFO] 10.244.0.55:34919 - 9845 "A IN registry.npmjs.org.default.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.0006244s
[INFO] 10.244.0.55:51317 - 38965 "A IN registry.npmjs.org.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.0001514s
[INFO] 10.244.0.55:51317 - 39265 "AAAA IN registry.npmjs.org.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.0001422s
[INFO] 10.244.0.55:52879 - 23045 "A IN registry.npmjs.org.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.0001907s
[INFO] 10.244.0.55:52879 - 23345 "AAAA IN registry.npmjs.org.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.0003114s
[INFO] 10.244.0.55:56511 - 15345 "A IN registry.npmjs.org. udp 36 false 512" NOERROR qr,rd,ra 444 0.0285202s
[INFO] 10.244.0.55:56511 - 15645 "AAAA IN registry.npmjs.org. udp 36 false 512" NOERROR qr,tc,rd,ra 1140 0.0502768s
[INFO] 10.244.0.55:38099 - 58430 "A IN redis-cluster-service.default.svc.cluster.local. udp 65 false 512" NOERROR qr,aa,rd 128 0.0002191s
[INFO] 10.244.0.55:47313 - 34760 "A IN redis-cluster-service.default.svc.cluster.local. udp 65 false 512" NOERROR qr,aa,rd 128 0.0004138s
[INFO] 10.244.0.56:58851 - 30385 "AAAA IN registry.npmjs.org.default.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.0003063s
[INFO] 10.244.0.56:58851 - 29985 "A IN registry.npmjs.org.default.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.0001498s
[INFO] 10.244.0.56:59292 - 55120 "AAAA IN registry.npmjs.org.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.0002198s
[INFO] 10.244.0.56:59292 - 54720 "A IN registry.npmjs.org.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.0003309s
[INFO] 10.244.0.56:49851 - 42195 "A IN registry.npmjs.org.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.0004385s
[INFO] 10.244.0.56:49851 - 42695 "AAAA IN registry.npmjs.org.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.0006251s
[INFO] 10.244.0.56:36357 - 63670 "AAAA IN registry.npmjs.org. udp 36 false 512" NOERROR qr,tc,rd,ra 1140 0.022433s
[INFO] 10.244.0.56:36357 - 63270 "A IN registry.npmjs.org. udp 36 false 512" NOERROR qr,rd,ra 444 0.0228258s
[INFO] 10.244.0.56:51866 - 52910 "A IN redis-cluster-service.default.svc.cluster.local. udp 65 false 512" NOERROR qr,aa,rd 128 0.0002455s
[INFO] 10.244.0.56:58664 - 3369 "A IN redis-cluster-service.default.svc.cluster.local. udp 65 false 512" NOERROR qr,aa,rd 128 0.0001821s

* 
* ==> coredns [fd753ca5c336] <==
* [INFO] 10.244.0.25:59647 - 54115 "A IN registry.npmjs.org. udp 36 false 512" NOERROR qr,rd,ra 444 0.0484772s
[INFO] 10.244.0.25:59647 - 54515 "AAAA IN registry.npmjs.org. udp 36 false 512" NOERROR qr,tc,rd,ra 1140 0.0508785s
[INFO] 10.244.0.31:59488 - 49355 "AAAA IN registry.npmjs.org.default.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.0004203s
[INFO] 10.244.0.31:59488 - 48355 "A IN registry.npmjs.org.default.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.000161s
[INFO] 10.244.0.31:48379 - 46665 "AAAA IN registry.npmjs.org.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.0002282s
[INFO] 10.244.0.31:48379 - 45965 "A IN registry.npmjs.org.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.0001238s
[INFO] 10.244.0.31:46733 - 18810 "AAAA IN registry.npmjs.org.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000212s
[INFO] 10.244.0.31:46733 - 18410 "A IN registry.npmjs.org.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.0001125s
[INFO] 10.244.0.31:50730 - 19590 "AAAA IN registry.npmjs.org. udp 36 false 512" NOERROR qr,aa,tc,rd,ra 1140 0.0023078s
[INFO] 10.244.0.31:50730 - 18990 "A IN registry.npmjs.org. udp 36 false 512" NOERROR qr,aa,rd,ra 444 0.0003178s
[INFO] 10.244.0.34:44155 - 5934 "AAAA IN registry.npmjs.org.default.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.0011557s
[INFO] 10.244.0.34:44155 - 4434 "A IN registry.npmjs.org.default.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.0014813s
[INFO] 10.244.0.34:39625 - 62920 "A IN registry.npmjs.org.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.0004485s
[INFO] 10.244.0.34:39625 - 64220 "AAAA IN registry.npmjs.org.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.0006218s
[INFO] 10.244.0.34:35844 - 20800 "AAAA IN registry.npmjs.org.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.0007906s
[INFO] 10.244.0.34:35844 - 19400 "A IN registry.npmjs.org.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.0011008s
[INFO] 10.244.0.34:53353 - 51480 "A IN registry.npmjs.org. udp 36 false 512" NOERROR qr,rd,ra 444 0.0191381s
[INFO] 10.244.0.34:53353 - 52580 "AAAA IN registry.npmjs.org. udp 36 false 512" NOERROR qr,tc,rd,ra 1140 0.0196956s
[INFO] 10.244.0.35:40314 - 14250 "AAAA IN registry.npmjs.org.default.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.0001807s
[INFO] 10.244.0.35:40314 - 13950 "A IN registry.npmjs.org.default.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.0002828s
[INFO] 10.244.0.35:54979 - 1864 "A IN registry.npmjs.org.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.0001565s
[INFO] 10.244.0.35:54979 - 2064 "AAAA IN registry.npmjs.org.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.0003565s
[INFO] 10.244.0.35:55547 - 32115 "AAAA IN registry.npmjs.org.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.0001826s
[INFO] 10.244.0.35:55547 - 31815 "A IN registry.npmjs.org.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.0002761s
[INFO] 10.244.0.35:42644 - 14565 "AAAA IN registry.npmjs.org. udp 36 false 512" NOERROR qr,aa,tc,rd,ra 1140 0.0002225s
[INFO] 10.244.0.35:42644 - 14265 "A IN registry.npmjs.org. udp 36 false 512" NOERROR qr,aa,rd,ra 444 0.0002593s
[INFO] 10.244.0.25:54509 - 43645 "AAAA IN registry.npmjs.org.default.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.0884151s
[INFO] 10.244.0.25:50802 - 58870 "A IN registry.npmjs.org.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.0006996s
[INFO] 10.244.0.25:50802 - 60970 "AAAA IN registry.npmjs.org.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.001108s
[INFO] 10.244.0.25:54509 - 42545 "A IN registry.npmjs.org.default.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.0922041s
[INFO] 10.244.0.25:32784 - 22605 "AAAA IN registry.npmjs.org.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.0006496s
[INFO] 10.244.0.25:32784 - 21305 "A IN registry.npmjs.org.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.0006803s
[INFO] 10.244.0.25:54181 - 34495 "A IN registry.npmjs.org. udp 36 false 512" NOERROR qr,rd,ra 444 0.1016997s
[INFO] 10.244.0.25:54181 - 35195 "AAAA IN registry.npmjs.org. udp 36 false 512" NOERROR qr,tc,rd,ra 1140 0.1068709s
[INFO] 10.244.0.34:48548 - 39720 "A IN registry.npmjs.org.default.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.0012549s
[INFO] 10.244.0.34:48548 - 41120 "AAAA IN registry.npmjs.org.default.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.0004226s
[INFO] 10.244.0.34:35808 - 31045 "AAAA IN registry.npmjs.org.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.0003638s
[INFO] 10.244.0.34:35808 - 29745 "A IN registry.npmjs.org.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.000398s
[INFO] 10.244.0.34:39264 - 10539 "AAAA IN registry.npmjs.org.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.0002853s
[INFO] 10.244.0.34:39264 - 9539 "A IN registry.npmjs.org.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.0003351s
[INFO] 10.244.0.34:43564 - 33080 "AAAA IN registry.npmjs.org. udp 36 false 512" NOERROR qr,aa,tc,rd,ra 1140 0.0066432s
[INFO] 10.244.0.34:43564 - 32080 "A IN registry.npmjs.org. udp 36 false 512" NOERROR qr,aa,rd,ra 444 0.0087434s
[INFO] 10.244.0.31:49293 - 43450 "AAAA IN registry.npmjs.org.default.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.0002353s
[INFO] 10.244.0.31:49293 - 42950 "A IN registry.npmjs.org.default.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.0003393s
[INFO] 10.244.0.31:59409 - 45695 "AAAA IN registry.npmjs.org.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.0002569s
[INFO] 10.244.0.31:59409 - 45195 "A IN registry.npmjs.org.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.0001168s
[INFO] 10.244.0.31:42401 - 32975 "AAAA IN registry.npmjs.org.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.0002218s
[INFO] 10.244.0.31:42401 - 32575 "A IN registry.npmjs.org.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.0003829s
[INFO] 10.244.0.31:60544 - 51055 "AAAA IN registry.npmjs.org. udp 36 false 512" NOERROR qr,tc,rd,ra 1140 0.0154384s
[INFO] 10.244.0.31:60544 - 50555 "A IN registry.npmjs.org. udp 36 false 512" NOERROR qr,rd,ra 444 0.015492s
[INFO] 10.244.0.35:33308 - 8494 "AAAA IN registry.npmjs.org.default.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.0007149s
[INFO] 10.244.0.35:33308 - 6894 "A IN registry.npmjs.org.default.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.0009122s
[INFO] 10.244.0.35:38633 - 27460 "AAAA IN registry.npmjs.org.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.0006356s
[INFO] 10.244.0.35:38633 - 25960 "A IN registry.npmjs.org.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.0011868s
[INFO] 10.244.0.35:40753 - 54105 "AAAA IN registry.npmjs.org.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000671s
[INFO] 10.244.0.35:40753 - 52905 "A IN registry.npmjs.org.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.001268s
[INFO] 10.244.0.35:55864 - 58580 "AAAA IN registry.npmjs.org. udp 36 false 512" NOERROR qr,aa,tc,rd,ra 1140 0.0011589s
[INFO] 10.244.0.35:55864 - 57380 "A IN registry.npmjs.org. udp 36 false 512" NOERROR qr,aa,rd,ra 444 0.001022s
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=ddac20b4b34a9c8c857fc602203b6ba2679794d3
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2023_03_06T19_23_06_0700
                    minikube.k8s.io/version=v1.29.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Mon, 06 Mar 2023 22:22:55 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Wed, 08 Mar 2023 01:50:33 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Wed, 08 Mar 2023 01:49:41 +0000   Mon, 06 Mar 2023 22:22:49 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Wed, 08 Mar 2023 01:49:41 +0000   Mon, 06 Mar 2023 22:22:49 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Wed, 08 Mar 2023 01:49:41 +0000   Mon, 06 Mar 2023 22:22:49 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Wed, 08 Mar 2023 01:49:41 +0000   Mon, 06 Mar 2023 22:23:07 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  263174212Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             12986872Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  263174212Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             12986872Ki
  pods:               110
System Info:
  Machine ID:                 f1a46cb41c9d45969ef9bdf4a48d9b28
  System UUID:                f1a46cb41c9d45969ef9bdf4a48d9b28
  Boot ID:                    4147feb8-7c82-47d0-afbc-bca03f53a0cd
  Kernel Version:             5.10.16.3-microsoft-standard-WSL2
  OS Image:                   Ubuntu 20.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://20.10.23
  Kubelet Version:            v1.26.1
  Kube-Proxy Version:         v1.26.1
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (20 in total)
  Namespace                   Name                                         CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                         ------------  ----------  ---------------  -------------  ---
  default                     client-deployment-6c5c65587d-62k4p           500m (6%!)(MISSING)     500m (6%!)(MISSING)   128Mi (1%!)(MISSING)       128Mi (1%!)(MISSING)     25h
  default                     client-deployment-6c5c65587d-gsbh2           500m (6%!)(MISSING)     500m (6%!)(MISSING)   128Mi (1%!)(MISSING)       128Mi (1%!)(MISSING)     25h
  default                     client-deployment-6c5c65587d-ph8nd           500m (6%!)(MISSING)     500m (6%!)(MISSING)   128Mi (1%!)(MISSING)       128Mi (1%!)(MISSING)     25h
  default                     postgress-deployment-7684fbb5d7-qkqpb        500m (6%!)(MISSING)     500m (6%!)(MISSING)   128Mi (1%!)(MISSING)       128Mi (1%!)(MISSING)     3h9m
  default                     redis-deployment-66ffdffc96-fk5cf            500m (6%!)(MISSING)     500m (6%!)(MISSING)   128Mi (1%!)(MISSING)       128Mi (1%!)(MISSING)     24h
  default                     server-deployment-784dfffddc-kt278           500m (6%!)(MISSING)     500m (6%!)(MISSING)   128Mi (1%!)(MISSING)       128Mi (1%!)(MISSING)     3h6m
  default                     server-deployment-784dfffddc-tmbhx           500m (6%!)(MISSING)     500m (6%!)(MISSING)   128Mi (1%!)(MISSING)       128Mi (1%!)(MISSING)     3h6m
  default                     server-deployment-784dfffddc-whtsj           500m (6%!)(MISSING)     500m (6%!)(MISSING)   128Mi (1%!)(MISSING)       128Mi (1%!)(MISSING)     3h6m
  default                     worker-deployment-7cb49c4559-9cdrd           500m (6%!)(MISSING)     500m (6%!)(MISSING)   128Mi (1%!)(MISSING)       128Mi (1%!)(MISSING)     3h6m
  ingress-nginx               ingress-nginx-controller-77669ff58-2fhgf     100m (1%!)(MISSING)     0 (0%!)(MISSING)      90Mi (0%!)(MISSING)        0 (0%!)(MISSING)         4m25s
  kube-system                 coredns-787d4945fb-j4h7l                     100m (1%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (1%!)(MISSING)     27h
  kube-system                 etcd-minikube                                100m (1%!)(MISSING)     0 (0%!)(MISSING)      100Mi (0%!)(MISSING)       0 (0%!)(MISSING)         27h
  kube-system                 kube-apiserver-minikube                      250m (3%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         27h
  kube-system                 kube-controller-manager-minikube             200m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         27h
  kube-system                 kube-proxy-5fv9k                             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         27h
  kube-system                 kube-scheduler-minikube                      100m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         27h
  kube-system                 metrics-server-5f8fcc9bb7-rzf8c              100m (1%!)(MISSING)     0 (0%!)(MISSING)      200Mi (1%!)(MISSING)       0 (0%!)(MISSING)         26h
  kube-system                 storage-provisioner                          0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         27h
  kubernetes-dashboard        dashboard-metrics-scraper-5c6664855-dlmpr    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         26h
  kubernetes-dashboard        kubernetes-dashboard-55c4cbbc7c-k669n        0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         26h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests      Limits
  --------           --------      ------
  cpu                5450m (68%!)(MISSING)   4500m (56%!)(MISSING)
  memory             1612Mi (12%!)(MISSING)  1322Mi (10%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)        0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)        0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)        0 (0%!)(MISSING)
Events:              <none>

* 
* ==> dmesg <==
* [Mar 7 23:06] WSL2: Performing memory compaction.
[Mar 7 23:07] WSL2: Performing memory compaction.
[Mar 7 23:09] WSL2: Performing memory compaction.
[Mar 7 23:12] WSL2: Performing memory compaction.
[Mar 7 23:17] WSL2: Performing memory compaction.
[Mar 7 23:18] WSL2: Performing memory compaction.
[Mar 7 23:20] WSL2: Performing memory compaction.
[Mar 7 23:22] WSL2: Performing memory compaction.
[Mar 7 23:24] WSL2: Performing memory compaction.
[Mar 7 23:25] WSL2: Performing memory compaction.
[Mar 7 23:26] WSL2: Performing memory compaction.
[Mar 7 23:31] WSL2: Performing memory compaction.
[Mar 7 23:34] WSL2: Performing memory compaction.
[Mar 7 23:35] WSL2: Performing memory compaction.
[Mar 7 23:37] WSL2: Performing memory compaction.
[Mar 7 23:39] WSL2: Performing memory compaction.
[Mar 7 23:41] WSL2: Performing memory compaction.
[Mar 7 23:42] WSL2: Performing memory compaction.
[Mar 7 23:46] WSL2: Performing memory compaction.
[Mar 7 23:49] WSL2: Performing memory compaction.
[Mar 7 23:52] WSL2: Performing memory compaction.
[Mar 7 23:54] WSL2: Performing memory compaction.
[Mar 7 23:56] WSL2: Performing memory compaction.
[Mar 7 23:58] WSL2: Performing memory compaction.
[Mar 8 00:00] WSL2: Performing memory compaction.
[Mar 8 00:04] WSL2: Performing memory compaction.
[Mar 8 00:10] WSL2: Performing memory compaction.
[Mar 8 00:11] WSL2: Performing memory compaction.
[Mar 8 00:13] WSL2: Performing memory compaction.
[Mar 8 00:17] WSL2: Performing memory compaction.
[Mar 8 00:23] WSL2: Performing memory compaction.
[Mar 8 00:25] WSL2: Performing memory compaction.
[Mar 8 00:27] WSL2: Performing memory compaction.
[Mar 8 00:35] WSL2: Performing memory compaction.
[Mar 8 00:38] WSL2: Performing memory compaction.
[Mar 8 00:39] WSL2: Performing memory compaction.
[Mar 8 00:41] WSL2: Performing memory compaction.
[Mar 8 00:43] WSL2: Performing memory compaction.
[Mar 8 00:44] WSL2: Performing memory compaction.
[Mar 8 00:47] WSL2: Performing memory compaction.
[Mar 8 00:50] WSL2: Performing memory compaction.
[Mar 8 00:52] WSL2: Performing memory compaction.
[Mar 8 00:57] WSL2: Performing memory compaction.
[Mar 8 00:58] WSL2: Performing memory compaction.
[Mar 8 00:59] WSL2: Performing memory compaction.
[Mar 8 01:07] WSL2: Performing memory compaction.
[Mar 8 01:13] WSL2: Performing memory compaction.
[Mar 8 01:16] WSL2: Performing memory compaction.
[Mar 8 01:22] WSL2: Performing memory compaction.
[Mar 8 01:24] WSL2: Performing memory compaction.
[Mar 8 01:28] WSL2: Performing memory compaction.
[Mar 8 01:30] WSL2: Performing memory compaction.
[Mar 8 01:31] WSL2: Performing memory compaction.
[Mar 8 01:35] WSL2: Performing memory compaction.
[Mar 8 01:36] WSL2: Performing memory compaction.
[Mar 8 01:40] WSL2: Performing memory compaction.
[Mar 8 01:42] WSL2: Performing memory compaction.
[Mar 8 01:43] WSL2: Performing memory compaction.
[Mar 8 01:44] WSL2: Performing memory compaction.
[Mar 8 01:48] WSL2: Performing memory compaction.

* 
* ==> etcd [39c5e37b7e11] <==
* {"level":"info","ts":"2023-03-08T01:45:50.315Z","caller":"traceutil/trace.go:171","msg":"trace[1404960600] range","detail":"{range_begin:/registry/serviceaccounts/ingress-nginx/ingress-nginx-admission; range_end:; response_count:0; response_revision:36871; }","duration":"282.5691ms","start":"2023-03-08T01:45:50.032Z","end":"2023-03-08T01:45:50.315Z","steps":["trace[1404960600] 'range keys from in-memory index tree'  (duration: 282.1943ms)"],"step_count":1}
{"level":"info","ts":"2023-03-08T01:45:50.440Z","caller":"traceutil/trace.go:171","msg":"trace[68119938] transaction","detail":"{read_only:false; response_revision:36872; number_of_response:1; }","duration":"106.1528ms","start":"2023-03-08T01:45:50.334Z","end":"2023-03-08T01:45:50.440Z","steps":["trace[68119938] 'process raft request'  (duration: 105.7373ms)"],"step_count":1}
{"level":"warn","ts":"2023-03-08T01:45:54.432Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"114.0069ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2023-03-08T01:45:54.433Z","caller":"traceutil/trace.go:171","msg":"trace[1816237537] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:36933; }","duration":"114.5626ms","start":"2023-03-08T01:45:54.318Z","end":"2023-03-08T01:45:54.433Z","steps":["trace[1816237537] 'range keys from in-memory index tree'  (duration: 113.9581ms)"],"step_count":1}
{"level":"info","ts":"2023-03-08T01:46:11.634Z","caller":"traceutil/trace.go:171","msg":"trace[1424212777] linearizableReadLoop","detail":"{readStateIndex:44670; appliedIndex:44668; }","duration":"210.2495ms","start":"2023-03-08T01:46:11.423Z","end":"2023-03-08T01:46:11.633Z","steps":["trace[1424212777] 'read index received'  (duration: 9.0912ms)","trace[1424212777] 'applied index is now lower than readState.Index'  (duration: 201.1563ms)"],"step_count":2}
{"level":"info","ts":"2023-03-08T01:46:11.634Z","caller":"traceutil/trace.go:171","msg":"trace[1433267832] transaction","detail":"{read_only:false; response_revision:37007; number_of_response:1; }","duration":"214.179ms","start":"2023-03-08T01:46:11.420Z","end":"2023-03-08T01:46:11.634Z","steps":["trace[1433267832] 'process raft request'  (duration: 199.0932ms)","trace[1433267832] 'compare'  (duration: 14.227ms)"],"step_count":2}
{"level":"info","ts":"2023-03-08T01:46:11.635Z","caller":"traceutil/trace.go:171","msg":"trace[932409741] transaction","detail":"{read_only:false; response_revision:37008; number_of_response:1; }","duration":"213.6756ms","start":"2023-03-08T01:46:11.421Z","end":"2023-03-08T01:46:11.635Z","steps":["trace[932409741] 'process raft request'  (duration: 212.47ms)"],"step_count":1}
{"level":"warn","ts":"2023-03-08T01:46:11.635Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"215.4431ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1112"}
{"level":"info","ts":"2023-03-08T01:46:11.635Z","caller":"traceutil/trace.go:171","msg":"trace[562481532] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:37008; }","duration":"215.5147ms","start":"2023-03-08T01:46:11.420Z","end":"2023-03-08T01:46:11.635Z","steps":["trace[562481532] 'agreement among raft nodes before linearized reading'  (duration: 215.2938ms)"],"step_count":1}
{"level":"info","ts":"2023-03-08T01:46:11.641Z","caller":"traceutil/trace.go:171","msg":"trace[1139883729] transaction","detail":"{read_only:false; response_revision:37009; number_of_response:1; }","duration":"208.3735ms","start":"2023-03-08T01:46:11.432Z","end":"2023-03-08T01:46:11.641Z","steps":["trace[1139883729] 'process raft request'  (duration: 207.9121ms)"],"step_count":1}
{"level":"warn","ts":"2023-03-08T01:46:12.145Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"105.9529ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/replicasets/ingress-nginx/ingress-nginx-controller-77669ff58\" ","response":"range_response_count:1 size:5255"}
{"level":"info","ts":"2023-03-08T01:46:12.145Z","caller":"traceutil/trace.go:171","msg":"trace[931476595] range","detail":"{range_begin:/registry/replicasets/ingress-nginx/ingress-nginx-controller-77669ff58; range_end:; response_count:1; response_revision:37029; }","duration":"106.4985ms","start":"2023-03-08T01:46:12.039Z","end":"2023-03-08T01:46:12.145Z","steps":["trace[931476595] 'agreement among raft nodes before linearized reading'  (duration: 105.6297ms)"],"step_count":1}
{"level":"info","ts":"2023-03-08T01:46:15.717Z","caller":"traceutil/trace.go:171","msg":"trace[577186785] linearizableReadLoop","detail":"{readStateIndex:44705; appliedIndex:44704; }","duration":"198.5432ms","start":"2023-03-08T01:46:15.518Z","end":"2023-03-08T01:46:15.717Z","steps":["trace[577186785] 'read index received'  (duration: 198.1874ms)","trace[577186785] 'applied index is now lower than readState.Index'  (duration: 352.9µs)"],"step_count":2}
{"level":"info","ts":"2023-03-08T01:46:15.717Z","caller":"traceutil/trace.go:171","msg":"trace[1739342312] transaction","detail":"{read_only:false; response_revision:37040; number_of_response:1; }","duration":"198.9928ms","start":"2023-03-08T01:46:15.518Z","end":"2023-03-08T01:46:15.717Z","steps":["trace[1739342312] 'process raft request'  (duration: 198.3421ms)"],"step_count":1}
{"level":"warn","ts":"2023-03-08T01:46:15.718Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"199.7007ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2023-03-08T01:46:15.718Z","caller":"traceutil/trace.go:171","msg":"trace[789605916] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:37040; }","duration":"199.9735ms","start":"2023-03-08T01:46:15.518Z","end":"2023-03-08T01:46:15.718Z","steps":["trace[789605916] 'agreement among raft nodes before linearized reading'  (duration: 199.4993ms)"],"step_count":1}
{"level":"warn","ts":"2023-03-08T01:46:15.941Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"122.1549ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/ingress-nginx/ingress-nginx-admission\" ","response":"range_response_count:1 size:982"}
{"level":"info","ts":"2023-03-08T01:46:15.941Z","caller":"traceutil/trace.go:171","msg":"trace[300608167] range","detail":"{range_begin:/registry/serviceaccounts/ingress-nginx/ingress-nginx-admission; range_end:; response_count:1; response_revision:37040; }","duration":"122.2182ms","start":"2023-03-08T01:46:15.819Z","end":"2023-03-08T01:46:15.941Z","steps":["trace[300608167] 'range keys from in-memory index tree'  (duration: 121.9975ms)"],"step_count":1}
{"level":"info","ts":"2023-03-08T01:46:16.216Z","caller":"traceutil/trace.go:171","msg":"trace[1689430545] transaction","detail":"{read_only:false; response_revision:37041; number_of_response:1; }","duration":"176.6337ms","start":"2023-03-08T01:46:16.039Z","end":"2023-03-08T01:46:16.216Z","steps":["trace[1689430545] 'process raft request'  (duration: 176.4247ms)"],"step_count":1}
{"level":"info","ts":"2023-03-08T01:46:16.514Z","caller":"traceutil/trace.go:171","msg":"trace[1718417353] transaction","detail":"{read_only:false; response_revision:37044; number_of_response:1; }","duration":"191.8313ms","start":"2023-03-08T01:46:16.322Z","end":"2023-03-08T01:46:16.513Z","steps":["trace[1718417353] 'process raft request'  (duration: 191.644ms)"],"step_count":1}
{"level":"info","ts":"2023-03-08T01:46:16.514Z","caller":"traceutil/trace.go:171","msg":"trace[113950849] transaction","detail":"{read_only:false; response_revision:37045; number_of_response:1; }","duration":"190.407ms","start":"2023-03-08T01:46:16.323Z","end":"2023-03-08T01:46:16.514Z","steps":["trace[113950849] 'process raft request'  (duration: 190.2033ms)"],"step_count":1}
{"level":"info","ts":"2023-03-08T01:46:16.714Z","caller":"traceutil/trace.go:171","msg":"trace[244787319] transaction","detail":"{read_only:false; response_revision:37047; number_of_response:1; }","duration":"198.3123ms","start":"2023-03-08T01:46:16.515Z","end":"2023-03-08T01:46:16.714Z","steps":["trace[244787319] 'process raft request'  (duration: 198.276ms)"],"step_count":1}
{"level":"info","ts":"2023-03-08T01:46:16.714Z","caller":"traceutil/trace.go:171","msg":"trace[1682153242] transaction","detail":"{read_only:false; response_revision:37046; number_of_response:1; }","duration":"379.6641ms","start":"2023-03-08T01:46:16.334Z","end":"2023-03-08T01:46:16.714Z","steps":["trace[1682153242] 'process raft request'  (duration: 280.5995ms)","trace[1682153242] 'compare'  (duration: 98.6547ms)"],"step_count":2}
{"level":"warn","ts":"2023-03-08T01:46:16.714Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-03-08T01:46:16.334Z","time spent":"379.7307ms","remote":"127.0.0.1:34820","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1506,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/endpointslices/ingress-nginx/ingress-nginx-controller-admission-xfx25\" mod_revision:37031 > success:<request_put:<key:\"/registry/endpointslices/ingress-nginx/ingress-nginx-controller-admission-xfx25\" value_size:1419 >> failure:<request_range:<key:\"/registry/endpointslices/ingress-nginx/ingress-nginx-controller-admission-xfx25\" > >"}
{"level":"info","ts":"2023-03-08T01:46:16.714Z","caller":"traceutil/trace.go:171","msg":"trace[128357673] linearizableReadLoop","detail":"{readStateIndex:44711; appliedIndex:44709; }","duration":"372.8701ms","start":"2023-03-08T01:46:16.341Z","end":"2023-03-08T01:46:16.714Z","steps":["trace[128357673] 'read index received'  (duration: 172.0105ms)","trace[128357673] 'applied index is now lower than readState.Index'  (duration: 200.8583ms)"],"step_count":2}
{"level":"warn","ts":"2023-03-08T01:46:16.715Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"373.1141ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/clusterroles/ingress-nginx\" ","response":"range_response_count:1 size:2037"}
{"level":"info","ts":"2023-03-08T01:46:16.715Z","caller":"traceutil/trace.go:171","msg":"trace[2109507156] range","detail":"{range_begin:/registry/clusterroles/ingress-nginx; range_end:; response_count:1; response_revision:37047; }","duration":"373.1444ms","start":"2023-03-08T01:46:16.341Z","end":"2023-03-08T01:46:16.715Z","steps":["trace[2109507156] 'agreement among raft nodes before linearized reading'  (duration: 373.0479ms)"],"step_count":1}
{"level":"warn","ts":"2023-03-08T01:46:16.715Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-03-08T01:46:16.341Z","time spent":"373.1929ms","remote":"127.0.0.1:34842","response type":"/etcdserverpb.KV/Range","request count":0,"request size":38,"response count":1,"response size":2061,"request content":"key:\"/registry/clusterroles/ingress-nginx\" "}
{"level":"warn","ts":"2023-03-08T01:46:16.732Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"193.5986ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/horizontalpodautoscalers/\" range_end:\"/registry/horizontalpodautoscalers0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2023-03-08T01:46:16.732Z","caller":"traceutil/trace.go:171","msg":"trace[151882089] range","detail":"{range_begin:/registry/horizontalpodautoscalers/; range_end:/registry/horizontalpodautoscalers0; response_count:0; response_revision:37048; }","duration":"193.6806ms","start":"2023-03-08T01:46:16.539Z","end":"2023-03-08T01:46:16.732Z","steps":["trace[151882089] 'agreement among raft nodes before linearized reading'  (duration: 193.5095ms)"],"step_count":1}
{"level":"warn","ts":"2023-03-08T01:46:17.215Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"194.3443ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/clusterrolebindings/ingress-nginx-admission\" ","response":"range_response_count:1 size:1380"}
{"level":"info","ts":"2023-03-08T01:46:17.215Z","caller":"traceutil/trace.go:171","msg":"trace[863653958] range","detail":"{range_begin:/registry/clusterrolebindings/ingress-nginx-admission; range_end:; response_count:1; response_revision:37048; }","duration":"194.4435ms","start":"2023-03-08T01:46:17.021Z","end":"2023-03-08T01:46:17.215Z","steps":["trace[863653958] 'range keys from in-memory index tree'  (duration: 194.0591ms)"],"step_count":1}
{"level":"warn","ts":"2023-03-08T01:46:17.526Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"102.8222ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/configmaps/ingress-nginx/udp-services\" ","response":"range_response_count:1 size:920"}
{"level":"info","ts":"2023-03-08T01:46:17.526Z","caller":"traceutil/trace.go:171","msg":"trace[1774579420] range","detail":"{range_begin:/registry/configmaps/ingress-nginx/udp-services; range_end:; response_count:1; response_revision:37049; }","duration":"102.9074ms","start":"2023-03-08T01:46:17.423Z","end":"2023-03-08T01:46:17.526Z","steps":["trace[1774579420] 'range keys from in-memory index tree'  (duration: 102.4759ms)"],"step_count":1}
{"level":"warn","ts":"2023-03-08T01:46:40.922Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"105.5445ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/validatingwebhookconfigurations/\" range_end:\"/registry/validatingwebhookconfigurations0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2023-03-08T01:46:40.923Z","caller":"traceutil/trace.go:171","msg":"trace[865611056] range","detail":"{range_begin:/registry/validatingwebhookconfigurations/; range_end:/registry/validatingwebhookconfigurations0; response_count:0; response_revision:37082; }","duration":"105.724ms","start":"2023-03-08T01:46:40.817Z","end":"2023-03-08T01:46:40.923Z","steps":["trace[865611056] 'count revisions from in-memory index tree'  (duration: 105.3556ms)"],"step_count":1}
{"level":"info","ts":"2023-03-08T01:47:33.841Z","caller":"traceutil/trace.go:171","msg":"trace[1399476208] transaction","detail":"{read_only:false; response_revision:37128; number_of_response:1; }","duration":"104.8987ms","start":"2023-03-08T01:47:33.736Z","end":"2023-03-08T01:47:33.841Z","steps":["trace[1399476208] 'process raft request'  (duration: 104.7099ms)"],"step_count":1}
{"level":"info","ts":"2023-03-08T01:48:12.017Z","caller":"traceutil/trace.go:171","msg":"trace[2116125228] linearizableReadLoop","detail":"{readStateIndex:44851; appliedIndex:44850; }","duration":"101.4472ms","start":"2023-03-08T01:48:11.916Z","end":"2023-03-08T01:48:12.017Z","steps":["trace[2116125228] 'read index received'  (duration: 101.3043ms)","trace[2116125228] 'applied index is now lower than readState.Index'  (duration: 142.1µs)"],"step_count":2}
{"level":"warn","ts":"2023-03-08T01:48:12.018Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"101.7047ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/ingress-nginx/\" range_end:\"/registry/pods/ingress-nginx0\" ","response":"range_response_count:3 size:13689"}
{"level":"info","ts":"2023-03-08T01:48:12.018Z","caller":"traceutil/trace.go:171","msg":"trace[38592999] range","detail":"{range_begin:/registry/pods/ingress-nginx/; range_end:/registry/pods/ingress-nginx0; response_count:3; response_revision:37162; }","duration":"101.7746ms","start":"2023-03-08T01:48:11.916Z","end":"2023-03-08T01:48:12.018Z","steps":["trace[38592999] 'agreement among raft nodes before linearized reading'  (duration: 101.5852ms)"],"step_count":1}
{"level":"warn","ts":"2023-03-08T01:48:41.214Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"287.968ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1112"}
{"level":"info","ts":"2023-03-08T01:48:41.214Z","caller":"traceutil/trace.go:171","msg":"trace[967891711] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:37189; }","duration":"288.2408ms","start":"2023-03-08T01:48:40.926Z","end":"2023-03-08T01:48:41.214Z","steps":["trace[967891711] 'range keys from in-memory index tree'  (duration: 287.3393ms)"],"step_count":1}
{"level":"info","ts":"2023-03-08T01:48:41.945Z","caller":"traceutil/trace.go:171","msg":"trace[1987441185] transaction","detail":"{read_only:false; response_revision:37191; number_of_response:1; }","duration":"113.5268ms","start":"2023-03-08T01:48:41.831Z","end":"2023-03-08T01:48:41.945Z","steps":["trace[1987441185] 'process raft request'  (duration: 82.7725ms)","trace[1987441185] 'get key's previous created_revision and leaseID' {req_type:put; key:/registry/masterleases/192.168.49.2; req_size:115; } (duration: 30.6233ms)"],"step_count":2}
{"level":"info","ts":"2023-03-08T01:48:50.023Z","caller":"traceutil/trace.go:171","msg":"trace[116205948] transaction","detail":"{read_only:false; response_revision:37198; number_of_response:1; }","duration":"100.3067ms","start":"2023-03-08T01:48:49.922Z","end":"2023-03-08T01:48:50.023Z","steps":["trace[116205948] 'process raft request'  (duration: 91.4335ms)"],"step_count":1}
{"level":"warn","ts":"2023-03-08T01:49:16.615Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"100.0392ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2023-03-08T01:49:16.615Z","caller":"traceutil/trace.go:171","msg":"trace[2128952714] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:37222; }","duration":"100.3574ms","start":"2023-03-08T01:49:16.515Z","end":"2023-03-08T01:49:16.615Z","steps":["trace[2128952714] 'range keys from in-memory index tree'  (duration: 99.7801ms)"],"step_count":1}
{"level":"info","ts":"2023-03-08T01:49:35.142Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":36806}
{"level":"info","ts":"2023-03-08T01:49:35.143Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":36806,"took":"938µs","hash":3565460762}
{"level":"info","ts":"2023-03-08T01:49:35.143Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3565460762,"revision":36806,"compact-revision":36392}
{"level":"info","ts":"2023-03-08T01:49:41.223Z","caller":"traceutil/trace.go:171","msg":"trace[1182762351] transaction","detail":"{read_only:false; response_revision:37245; number_of_response:1; }","duration":"103.9521ms","start":"2023-03-08T01:49:41.119Z","end":"2023-03-08T01:49:41.223Z","steps":["trace[1182762351] 'process raft request'  (duration: 94.849ms)"],"step_count":1}
{"level":"info","ts":"2023-03-08T01:49:42.529Z","caller":"traceutil/trace.go:171","msg":"trace[1149736263] transaction","detail":"{read_only:false; response_revision:37248; number_of_response:1; }","duration":"112.7173ms","start":"2023-03-08T01:49:42.416Z","end":"2023-03-08T01:49:42.529Z","steps":["trace[1149736263] 'process raft request'  (duration: 112.1277ms)"],"step_count":1}
{"level":"info","ts":"2023-03-08T01:50:22.131Z","caller":"traceutil/trace.go:171","msg":"trace[340024084] transaction","detail":"{read_only:false; response_revision:37284; number_of_response:1; }","duration":"115.7341ms","start":"2023-03-08T01:50:22.015Z","end":"2023-03-08T01:50:22.131Z","steps":["trace[340024084] 'process raft request'  (duration: 112.6647ms)"],"step_count":1}
{"level":"warn","ts":"2023-03-08T01:50:22.131Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"115.5833ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1112"}
{"level":"info","ts":"2023-03-08T01:50:22.131Z","caller":"traceutil/trace.go:171","msg":"trace[2128136477] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:37284; }","duration":"115.632ms","start":"2023-03-08T01:50:22.015Z","end":"2023-03-08T01:50:22.131Z","steps":["trace[2128136477] 'agreement among raft nodes before linearized reading'  (duration: 115.4747ms)"],"step_count":1}
{"level":"info","ts":"2023-03-08T01:50:22.131Z","caller":"traceutil/trace.go:171","msg":"trace[1081762881] linearizableReadLoop","detail":"{readStateIndex:45000; appliedIndex:44999; }","duration":"115.3883ms","start":"2023-03-08T01:50:22.015Z","end":"2023-03-08T01:50:22.131Z","steps":["trace[1081762881] 'read index received'  (duration: 112.2701ms)","trace[1081762881] 'applied index is now lower than readState.Index'  (duration: 3.1168ms)"],"step_count":2}
{"level":"info","ts":"2023-03-08T01:50:34.215Z","caller":"traceutil/trace.go:171","msg":"trace[1636448303] transaction","detail":"{read_only:false; response_revision:37295; number_of_response:1; }","duration":"199.0567ms","start":"2023-03-08T01:50:34.016Z","end":"2023-03-08T01:50:34.215Z","steps":["trace[1636448303] 'process raft request'  (duration: 198.7173ms)"],"step_count":1}
{"level":"warn","ts":"2023-03-08T01:50:34.421Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"103.811ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2023-03-08T01:50:34.421Z","caller":"traceutil/trace.go:171","msg":"trace[367133752] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:37295; }","duration":"104.1916ms","start":"2023-03-08T01:50:34.317Z","end":"2023-03-08T01:50:34.421Z","steps":["trace[367133752] 'range keys from in-memory index tree'  (duration: 103.772ms)"],"step_count":1}
{"level":"warn","ts":"2023-03-08T01:50:34.422Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"105.0066ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1112"}
{"level":"info","ts":"2023-03-08T01:50:34.423Z","caller":"traceutil/trace.go:171","msg":"trace[1433657632] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:37295; }","duration":"105.1837ms","start":"2023-03-08T01:50:34.317Z","end":"2023-03-08T01:50:34.423Z","steps":["trace[1433657632] 'range keys from in-memory index tree'  (duration: 104.4831ms)"],"step_count":1}

* 
* ==> etcd [5f204ad627bb] <==
* {"level":"info","ts":"2023-03-07T14:25:36.050Z","caller":"traceutil/trace.go:171","msg":"trace[2133875277] range","detail":"{range_begin:/registry/prioritylevelconfigurations/; range_end:/registry/prioritylevelconfigurations0; response_count:0; response_revision:21144; }","duration":"135.8129ms","start":"2023-03-07T14:25:35.914Z","end":"2023-03-07T14:25:36.050Z","steps":["trace[2133875277] 'agreement among raft nodes before linearized reading'  (duration: 135.7205ms)"],"step_count":1}
{"level":"warn","ts":"2023-03-07T14:25:37.776Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"1.6280722s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/ingress-nginx/\" range_end:\"/registry/pods/ingress-nginx0\" ","response":"range_response_count:3 size:13577"}
{"level":"info","ts":"2023-03-07T14:25:37.776Z","caller":"traceutil/trace.go:171","msg":"trace[1707616981] range","detail":"{range_begin:/registry/pods/ingress-nginx/; range_end:/registry/pods/ingress-nginx0; response_count:3; response_revision:21144; }","duration":"1.6282317s","start":"2023-03-07T14:25:36.148Z","end":"2023-03-07T14:25:37.776Z","steps":["trace[1707616981] 'range keys from in-memory index tree'  (duration: 1.6278531s)"],"step_count":1}
{"level":"warn","ts":"2023-03-07T14:25:37.776Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-03-07T14:25:36.148Z","time spent":"1.6283258s","remote":"127.0.0.1:34976","response type":"/etcdserverpb.KV/Range","request count":0,"request size":62,"response count":3,"response size":13601,"request content":"key:\"/registry/pods/ingress-nginx/\" range_end:\"/registry/pods/ingress-nginx0\" "}
{"level":"warn","ts":"2023-03-07T14:25:37.776Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"1.5733442s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1111"}
{"level":"info","ts":"2023-03-07T14:25:37.776Z","caller":"traceutil/trace.go:171","msg":"trace[902196495] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:21144; }","duration":"1.5734221s","start":"2023-03-07T14:25:36.203Z","end":"2023-03-07T14:25:37.776Z","steps":["trace[902196495] 'range keys from in-memory index tree'  (duration: 1.5732313s)"],"step_count":1}
{"level":"warn","ts":"2023-03-07T14:25:37.776Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-03-07T14:25:36.203Z","time spent":"1.573529s","remote":"127.0.0.1:34972","response type":"/etcdserverpb.KV/Range","request count":0,"request size":67,"response count":1,"response size":1135,"request content":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" "}
{"level":"warn","ts":"2023-03-07T14:25:37.776Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"313.6005ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/persistentvolumeclaims/\" range_end:\"/registry/persistentvolumeclaims0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"warn","ts":"2023-03-07T14:25:37.776Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"433.7667ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2023-03-07T14:25:37.776Z","caller":"traceutil/trace.go:171","msg":"trace[68968553] range","detail":"{range_begin:/registry/persistentvolumeclaims/; range_end:/registry/persistentvolumeclaims0; response_count:0; response_revision:21144; }","duration":"313.6689ms","start":"2023-03-07T14:25:37.463Z","end":"2023-03-07T14:25:37.776Z","steps":["trace[68968553] 'count revisions from in-memory index tree'  (duration: 313.4155ms)"],"step_count":1}
{"level":"info","ts":"2023-03-07T14:25:37.776Z","caller":"traceutil/trace.go:171","msg":"trace[1537430075] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:21144; }","duration":"433.8884ms","start":"2023-03-07T14:25:37.342Z","end":"2023-03-07T14:25:37.776Z","steps":["trace[1537430075] 'range keys from in-memory index tree'  (duration: 433.6837ms)"],"step_count":1}
{"level":"warn","ts":"2023-03-07T14:25:37.776Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-03-07T14:25:37.463Z","time spent":"313.828ms","remote":"127.0.0.1:34966","response type":"/etcdserverpb.KV/Range","request count":0,"request size":72,"response count":1,"response size":32,"request content":"key:\"/registry/persistentvolumeclaims/\" range_end:\"/registry/persistentvolumeclaims0\" count_only:true "}
{"level":"warn","ts":"2023-03-07T14:25:37.776Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-03-07T14:25:37.342Z","time spent":"433.9723ms","remote":"127.0.0.1:35030","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":30,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2023-03-07T14:25:38.282Z","caller":"etcdserver/v3_server.go:840","msg":"waiting for ReadIndex response took too long, retrying","sent-request-id":8128019569296731264,"retry-timeout":"500ms"}
{"level":"warn","ts":"2023-03-07T14:25:38.783Z","caller":"etcdserver/v3_server.go:840","msg":"waiting for ReadIndex response took too long, retrying","sent-request-id":8128019569296731264,"retry-timeout":"500ms"}
{"level":"warn","ts":"2023-03-07T14:25:39.285Z","caller":"etcdserver/v3_server.go:840","msg":"waiting for ReadIndex response took too long, retrying","sent-request-id":8128019569296731264,"retry-timeout":"500ms"}
{"level":"warn","ts":"2023-03-07T14:25:39.428Z","caller":"wal/wal.go:805","msg":"slow fdatasync","took":"1.6458371s","expected-duration":"1s"}
{"level":"info","ts":"2023-03-07T14:25:39.428Z","caller":"traceutil/trace.go:171","msg":"trace[250745491] linearizableReadLoop","detail":"{readStateIndex:25592; appliedIndex:25591; }","duration":"1.6462066s","start":"2023-03-07T14:25:37.782Z","end":"2023-03-07T14:25:39.428Z","steps":["trace[250745491] 'read index received'  (duration: 1.6460192s)","trace[250745491] 'applied index is now lower than readState.Index'  (duration: 187µs)"],"step_count":2}
{"level":"info","ts":"2023-03-07T14:25:39.429Z","caller":"traceutil/trace.go:171","msg":"trace[829624128] transaction","detail":"{read_only:false; response_revision:21145; number_of_response:1; }","duration":"1.6464993s","start":"2023-03-07T14:25:37.782Z","end":"2023-03-07T14:25:39.429Z","steps":["trace[829624128] 'process raft request'  (duration: 1.646319s)"],"step_count":1}
{"level":"warn","ts":"2023-03-07T14:25:39.429Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"1.646376s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/minions/minikube\" ","response":"range_response_count:1 size:5794"}
{"level":"info","ts":"2023-03-07T14:25:39.429Z","caller":"traceutil/trace.go:171","msg":"trace[213633392] range","detail":"{range_begin:/registry/minions/minikube; range_end:; response_count:1; response_revision:21145; }","duration":"1.6464069s","start":"2023-03-07T14:25:37.782Z","end":"2023-03-07T14:25:39.429Z","steps":["trace[213633392] 'agreement among raft nodes before linearized reading'  (duration: 1.6463222s)"],"step_count":1}
{"level":"warn","ts":"2023-03-07T14:25:39.429Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-03-07T14:25:37.782Z","time spent":"1.6464523s","remote":"127.0.0.1:34974","response type":"/etcdserverpb.KV/Range","request count":0,"request size":28,"response count":1,"response size":5818,"request content":"key:\"/registry/minions/minikube\" "}
{"level":"warn","ts":"2023-03-07T14:25:39.429Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"1.4193281s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/\" range_end:\"/registry/pods0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"warn","ts":"2023-03-07T14:25:39.429Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-03-07T14:25:37.782Z","time spent":"1.646644s","remote":"127.0.0.1:34972","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:21144 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2023-03-07T14:25:39.429Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"231.3287ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/flowschemas/\" range_end:\"/registry/flowschemas0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"warn","ts":"2023-03-07T14:25:39.429Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"1.0855796s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2023-03-07T14:25:39.429Z","caller":"traceutil/trace.go:171","msg":"trace[56899792] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:21145; }","duration":"1.0856088s","start":"2023-03-07T14:25:38.343Z","end":"2023-03-07T14:25:39.429Z","steps":["trace[56899792] 'agreement among raft nodes before linearized reading'  (duration: 1.0855646s)"],"step_count":1}
{"level":"info","ts":"2023-03-07T14:25:39.429Z","caller":"traceutil/trace.go:171","msg":"trace[592277528] range","detail":"{range_begin:/registry/pods/; range_end:/registry/pods0; response_count:0; response_revision:21145; }","duration":"1.4193686s","start":"2023-03-07T14:25:38.009Z","end":"2023-03-07T14:25:39.429Z","steps":["trace[592277528] 'agreement among raft nodes before linearized reading'  (duration: 1.4193019s)"],"step_count":1}
{"level":"warn","ts":"2023-03-07T14:25:39.429Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-03-07T14:25:38.343Z","time spent":"1.0856456s","remote":"127.0.0.1:35030","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":30,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2023-03-07T14:25:39.429Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-03-07T14:25:38.009Z","time spent":"1.4194691s","remote":"127.0.0.1:34976","response type":"/etcdserverpb.KV/Range","request count":0,"request size":36,"response count":22,"response size":32,"request content":"key:\"/registry/pods/\" range_end:\"/registry/pods0\" count_only:true "}
{"level":"warn","ts":"2023-03-07T14:25:39.429Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"939.1021ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/leases/ingress-nginx/ingress-nginx-leader\" ","response":"range_response_count:1 size:503"}
{"level":"info","ts":"2023-03-07T14:25:39.429Z","caller":"traceutil/trace.go:171","msg":"trace[1580182750] range","detail":"{range_begin:/registry/leases/ingress-nginx/ingress-nginx-leader; range_end:; response_count:1; response_revision:21145; }","duration":"939.2902ms","start":"2023-03-07T14:25:38.490Z","end":"2023-03-07T14:25:39.429Z","steps":["trace[1580182750] 'agreement among raft nodes before linearized reading'  (duration: 939.0569ms)"],"step_count":1}
{"level":"warn","ts":"2023-03-07T14:25:39.429Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-03-07T14:25:38.490Z","time spent":"939.3587ms","remote":"127.0.0.1:35004","response type":"/etcdserverpb.KV/Range","request count":0,"request size":53,"response count":1,"response size":527,"request content":"key:\"/registry/leases/ingress-nginx/ingress-nginx-leader\" "}
{"level":"info","ts":"2023-03-07T14:25:39.429Z","caller":"traceutil/trace.go:171","msg":"trace[1559949370] range","detail":"{range_begin:/registry/flowschemas/; range_end:/registry/flowschemas0; response_count:0; response_revision:21145; }","duration":"231.3617ms","start":"2023-03-07T14:25:39.197Z","end":"2023-03-07T14:25:39.429Z","steps":["trace[1559949370] 'agreement among raft nodes before linearized reading'  (duration: 231.3109ms)"],"step_count":1}
{"level":"info","ts":"2023-03-07T14:25:39.585Z","caller":"traceutil/trace.go:171","msg":"trace[712521648] linearizableReadLoop","detail":"{readStateIndex:25593; appliedIndex:25592; }","duration":"147.7931ms","start":"2023-03-07T14:25:39.437Z","end":"2023-03-07T14:25:39.584Z","steps":["trace[712521648] 'read index received'  (duration: 141.0837ms)","trace[712521648] 'applied index is now lower than readState.Index'  (duration: 6.7084ms)"],"step_count":2}
{"level":"warn","ts":"2023-03-07T14:25:39.585Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"147.9457ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2023-03-07T14:25:39.585Z","caller":"traceutil/trace.go:171","msg":"trace[1585235255] transaction","detail":"{read_only:false; response_revision:21146; number_of_response:1; }","duration":"151.8034ms","start":"2023-03-07T14:25:39.433Z","end":"2023-03-07T14:25:39.585Z","steps":["trace[1585235255] 'process raft request'  (duration: 144.9847ms)"],"step_count":1}
{"level":"info","ts":"2023-03-07T14:25:39.585Z","caller":"traceutil/trace.go:171","msg":"trace[895494852] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:21146; }","duration":"148.0025ms","start":"2023-03-07T14:25:39.437Z","end":"2023-03-07T14:25:39.585Z","steps":["trace[895494852] 'agreement among raft nodes before linearized reading'  (duration: 147.9227ms)"],"step_count":1}
{"level":"info","ts":"2023-03-07T14:27:15.485Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":20959}
{"level":"info","ts":"2023-03-07T14:27:15.486Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":20959,"took":"1.0892ms","hash":1537035272}
{"level":"info","ts":"2023-03-07T14:27:15.486Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1537035272,"revision":20959,"compact-revision":20680}
{"level":"info","ts":"2023-03-07T14:29:47.803Z","caller":"traceutil/trace.go:171","msg":"trace[1436516475] transaction","detail":"{read_only:false; response_revision:21377; number_of_response:1; }","duration":"115.8495ms","start":"2023-03-07T14:29:47.688Z","end":"2023-03-07T14:29:47.803Z","steps":["trace[1436516475] 'process raft request'  (duration: 21.1041ms)","trace[1436516475] 'compare'  (duration: 94.0742ms)"],"step_count":2}
{"level":"info","ts":"2023-03-07T14:30:02.953Z","caller":"traceutil/trace.go:171","msg":"trace[1387227031] transaction","detail":"{read_only:false; response_revision:21391; number_of_response:1; }","duration":"121.3943ms","start":"2023-03-07T14:30:02.832Z","end":"2023-03-07T14:30:02.953Z","steps":["trace[1387227031] 'process raft request'  (duration: 121.1319ms)"],"step_count":1}
{"level":"info","ts":"2023-03-07T14:30:10.885Z","caller":"traceutil/trace.go:171","msg":"trace[1031899269] transaction","detail":"{read_only:false; response_revision:21399; number_of_response:1; }","duration":"179.5472ms","start":"2023-03-07T14:30:10.705Z","end":"2023-03-07T14:30:10.885Z","steps":["trace[1031899269] 'process raft request'  (duration: 97.288ms)","trace[1031899269] 'compare'  (duration: 81.9494ms)"],"step_count":2}
{"level":"info","ts":"2023-03-07T14:32:15.501Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":21236}
{"level":"info","ts":"2023-03-07T14:32:15.502Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":21236,"took":"708.9µs","hash":584959830}
{"level":"info","ts":"2023-03-07T14:32:15.502Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":584959830,"revision":21236,"compact-revision":20959}
{"level":"info","ts":"2023-03-07T14:33:53.097Z","caller":"traceutil/trace.go:171","msg":"trace[226266842] transaction","detail":"{read_only:false; response_revision:21606; number_of_response:1; }","duration":"199.6963ms","start":"2023-03-07T14:33:52.897Z","end":"2023-03-07T14:33:53.097Z","steps":["trace[226266842] 'process raft request'  (duration: 105.5317ms)","trace[226266842] 'compare'  (duration: 93.8837ms)"],"step_count":2}
{"level":"info","ts":"2023-03-07T14:34:27.604Z","caller":"traceutil/trace.go:171","msg":"trace[1016309532] transaction","detail":"{read_only:false; response_revision:21637; number_of_response:1; }","duration":"112.8446ms","start":"2023-03-07T14:34:27.491Z","end":"2023-03-07T14:34:27.604Z","steps":["trace[1016309532] 'process raft request'  (duration: 110.3582ms)"],"step_count":1}
{"level":"info","ts":"2023-03-07T14:37:15.514Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":21514}
{"level":"info","ts":"2023-03-07T14:37:15.515Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":21514,"took":"620.1µs","hash":2660856708}
{"level":"info","ts":"2023-03-07T14:37:15.515Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2660856708,"revision":21514,"compact-revision":21236}
{"level":"warn","ts":"2023-03-07T14:37:47.805Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"118.485ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/csistoragecapacities/\" range_end:\"/registry/csistoragecapacities0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2023-03-07T14:37:47.805Z","caller":"traceutil/trace.go:171","msg":"trace[839054898] range","detail":"{range_begin:/registry/csistoragecapacities/; range_end:/registry/csistoragecapacities0; response_count:0; response_revision:21824; }","duration":"118.6763ms","start":"2023-03-07T14:37:47.686Z","end":"2023-03-07T14:37:47.805Z","steps":["trace[839054898] 'count revisions from in-memory index tree'  (duration: 118.3459ms)"],"step_count":1}
{"level":"info","ts":"2023-03-07T14:38:02.886Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2023-03-07T14:38:02.890Z","caller":"embed/etcd.go:373","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"info","ts":"2023-03-07T14:38:03.198Z","caller":"etcdserver/server.go:1465","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2023-03-07T14:38:03.406Z","caller":"embed/etcd.go:568","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2023-03-07T14:38:03.496Z","caller":"embed/etcd.go:573","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2023-03-07T14:38:03.497Z","caller":"embed/etcd.go:375","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}

* 
* ==> kernel <==
*  01:50:36 up 13:49,  0 users,  load average: 1.27, 2.17, 2.34
Linux minikube 5.10.16.3-microsoft-standard-WSL2 #1 SMP Fri Apr 2 22:23:49 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 20.04.5 LTS"

* 
* ==> kube-apiserver [6ff83d33cdac] <==
* I0307 23:52:19.816654       1 trace.go:219] Trace[2138092990]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (07-Mar-2023 23:52:18.425) (total time: 1391ms):
Trace[2138092990]: ---"Transaction prepared" 66ms (23:52:18.507)
Trace[2138092990]: ---"Txn call completed" 1309ms (23:52:19.816)
Trace[2138092990]: [1.3911728s] [1.3911728s] END
I0307 23:52:19.818175       1 trace.go:219] Trace[1665346523]: "Get" accept:application/json, */*,audit-id:034e5729-16b9-4970-aaea-1d50cc1a7006,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:GET (07-Mar-2023 23:52:18.579) (total time: 1239ms):
Trace[1665346523]: ---"About to write a response" 1238ms (23:52:19.818)
Trace[1665346523]: [1.2391017s] [1.2391017s] END
I0308 00:18:48.611420       1 trace.go:219] Trace[789234728]: "Update" accept:application/json, */*,audit-id:98552bff-c8b2-425c-9746-e89420a4ab4f,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (08-Mar-2023 00:18:48.036) (total time: 575ms):
Trace[789234728]: ["GuaranteedUpdate etcd3" audit-id:98552bff-c8b2-425c-9746-e89420a4ab4f,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 575ms (00:18:48.036)
Trace[789234728]:  ---"Txn call completed" 574ms (00:18:48.611)]
Trace[789234728]: [575.3639ms] [575.3639ms] END
I0308 00:29:51.823502       1 trace.go:219] Trace[274895371]: "Update" accept:application/json, */*,audit-id:2031a62c-1c8a-47b9-82a7-5f10f22399d1,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (08-Mar-2023 00:29:51.231) (total time: 591ms):
Trace[274895371]: ["GuaranteedUpdate etcd3" audit-id:2031a62c-1c8a-47b9-82a7-5f10f22399d1,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 590ms (00:29:51.232)
Trace[274895371]:  ---"Txn call completed" 588ms (00:29:51.823)]
Trace[274895371]: [591.4587ms] [591.4587ms] END
I0308 00:58:10.531188       1 trace.go:219] Trace[1903965999]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (08-Mar-2023 00:58:10.028) (total time: 502ms):
Trace[1903965999]: ---"initial value restored" 286ms (00:58:10.315)
Trace[1903965999]: ---"Transaction prepared" 98ms (00:58:10.413)
Trace[1903965999]: ---"Txn call completed" 117ms (00:58:10.531)
Trace[1903965999]: [502.4051ms] [502.4051ms] END
I0308 01:06:41.014763       1 trace.go:219] Trace[576228773]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (08-Mar-2023 01:06:40.323) (total time: 608ms):
Trace[576228773]: ---"initial value restored" 93ms (01:06:40.416)
Trace[576228773]: ---"Transaction prepared" 120ms (01:06:40.537)
Trace[576228773]: ---"Txn call completed" 394ms (01:06:40.931)
Trace[576228773]: [608.9429ms] [608.9429ms] END
I0308 01:24:44.516480       1 trace.go:219] Trace[1062020155]: "Patch" accept:application/json,audit-id:45225283-aada-441e-8fea-8adac12e7be6,client:192.168.49.1,protocol:HTTP/2.0,resource:deployments,scope:resource,url:/apis/apps/v1/namespaces/ingress-nginx/deployments/ingress-nginx-controller,user-agent:kubectl/v1.26.2 (linux/amd64) kubernetes/fc04e73,verb:PATCH (08-Mar-2023 01:24:43.937) (total time: 578ms):
Trace[1062020155]: ["GuaranteedUpdate etcd3" audit-id:45225283-aada-441e-8fea-8adac12e7be6,key:/deployments/ingress-nginx/ingress-nginx-controller,type:*apps.Deployment,resource:deployments.apps 577ms (01:24:43.938)
Trace[1062020155]:  ---"About to Encode" 78ms (01:24:44.017)
Trace[1062020155]:  ---"Txn call completed" 497ms (01:24:44.515)]
Trace[1062020155]: ---"About to check admission control" 77ms (01:24:44.016)
Trace[1062020155]: ---"Object stored in database" 498ms (01:24:44.515)
Trace[1062020155]: [578.6421ms] [578.6421ms] END
I0308 01:27:17.870782       1 trace.go:219] Trace[544000173]: "Delete" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:4b1a07eb-a508-427c-94c5-04c857a4807f,client:192.168.49.2,protocol:HTTP/2.0,resource:pods,scope:namespace,url:/api/v1/namespaces/ingress-nginx/pods,user-agent:kube-controller-manager/v1.26.1 (linux/amd64) kubernetes/8f94681/system:serviceaccount:kube-system:namespace-controller,verb:DELETE (08-Mar-2023 01:27:17.335) (total time: 535ms):
Trace[544000173]: ["List(recursive=true) etcd3" audit-id:4b1a07eb-a508-427c-94c5-04c857a4807f,key:/pods/ingress-nginx,resourceVersion:,resourceVersionMatch:,limit:0,continue: 534ms (01:27:17.335)]
Trace[544000173]: [535.0589ms] [535.0589ms] END
E0308 01:27:18.323335       1 authentication.go:63] "Unable to authenticate the request" err="[invalid bearer token, serviceaccounts \"ingress-nginx\" not found]"
I0308 01:33:08.769407       1 controller.go:615] quota admission added evaluator for: namespaces
I0308 01:33:08.987931       1 alloc.go:327] "allocated clusterIPs" service="ingress-nginx/ingress-nginx-controller" clusterIPs=map[IPv4:10.100.108.27]
I0308 01:33:09.050818       1 alloc.go:327] "allocated clusterIPs" service="ingress-nginx/ingress-nginx-controller-admission" clusterIPs=map[IPv4:10.104.171.48]
I0308 01:33:09.141280       1 controller.go:615] quota admission added evaluator for: jobs.batch
E0308 01:36:10.180788       1 authentication.go:63] "Unable to authenticate the request" err="[invalid bearer token, serviceaccounts \"ingress-nginx\" not found]"
I0308 01:36:15.486011       1 trace.go:219] Trace[1176880934]: "Delete" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:78ebe8c7-ad1c-4d3e-a84b-1f0e84e74a9a,client:192.168.49.2,protocol:HTTP/2.0,resource:events,scope:namespace,url:/apis/events.k8s.io/v1/namespaces/ingress-nginx/events,user-agent:kube-controller-manager/v1.26.1 (linux/amd64) kubernetes/8f94681/system:serviceaccount:kube-system:namespace-controller,verb:DELETE (08-Mar-2023 01:36:14.928) (total time: 557ms):
Trace[1176880934]: ---"About to write a response" 554ms (01:36:15.484)
Trace[1176880934]: [557.8531ms] [557.8531ms] END
I0308 01:36:21.396154       1 alloc.go:327] "allocated clusterIPs" service="ingress-nginx/ingress-nginx-controller" clusterIPs=map[IPv4:10.109.86.59]
I0308 01:36:21.444508       1 alloc.go:327] "allocated clusterIPs" service="ingress-nginx/ingress-nginx-controller-admission" clusterIPs=map[IPv4:10.98.154.48]
I0308 01:37:22.099772       1 trace.go:219] Trace[15355299]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (08-Mar-2023 01:37:21.449) (total time: 650ms):
Trace[15355299]: ---"Transaction prepared" 335ms (01:37:21.784)
Trace[15355299]: ---"Txn call completed" 314ms (01:37:22.099)
Trace[15355299]: [650.3842ms] [650.3842ms] END
I0308 01:38:00.114379       1 trace.go:219] Trace[1368371161]: "Delete" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:c602c33a-436c-4621-b8b4-02ac7125f95f,client:192.168.49.2,protocol:HTTP/2.0,resource:configmaps,scope:namespace,url:/api/v1/namespaces/ingress-nginx/configmaps,user-agent:kube-controller-manager/v1.26.1 (linux/amd64) kubernetes/8f94681/system:serviceaccount:kube-system:namespace-controller,verb:DELETE (08-Mar-2023 01:37:59.515) (total time: 598ms):
Trace[1368371161]: ---"About to write a response" 593ms (01:38:00.114)
Trace[1368371161]: [598.8451ms] [598.8451ms] END
I0308 01:38:01.325822       1 trace.go:219] Trace[1541579251]: "Delete" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:82d33261-55bf-48d1-9e8d-fa48ab2766ac,client:192.168.49.2,protocol:HTTP/2.0,resource:events,scope:namespace,url:/apis/events.k8s.io/v1/namespaces/ingress-nginx/events,user-agent:kube-controller-manager/v1.26.1 (linux/amd64) kubernetes/8f94681/system:serviceaccount:kube-system:namespace-controller,verb:DELETE (08-Mar-2023 01:38:00.732) (total time: 591ms):
Trace[1541579251]: ---"About to write a response" 580ms (01:38:01.316)
Trace[1541579251]: [591.2499ms] [591.2499ms] END
I0308 01:38:15.179448       1 alloc.go:327] "allocated clusterIPs" service="ingress-nginx/ingress-nginx-controller" clusterIPs=map[IPv4:10.111.236.20]
I0308 01:38:15.211716       1 alloc.go:327] "allocated clusterIPs" service="ingress-nginx/ingress-nginx-controller-admission" clusterIPs=map[IPv4:10.96.125.193]
I0308 01:45:51.296271       1 alloc.go:327] "allocated clusterIPs" service="ingress-nginx/ingress-nginx-controller" clusterIPs=map[IPv4:10.110.238.254]
I0308 01:45:51.328227       1 alloc.go:327] "allocated clusterIPs" service="ingress-nginx/ingress-nginx-controller-admission" clusterIPs=map[IPv4:10.100.217.3]

* 
* ==> kube-apiserver [e033fc549989] <==
*   "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0307 14:38:04.088868       1 logging.go:59] [core] [Channel #145 SubChannel #146] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0307 14:38:04.089031       1 logging.go:59] [core] [Channel #40 SubChannel #41] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0307 14:38:04.089075       1 logging.go:59] [core] [Channel #151 SubChannel #152] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0307 14:38:04.089226       1 logging.go:59] [core] [Channel #43 SubChannel #44] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0307 14:38:04.089234       1 logging.go:59] [core] [Channel #142 SubChannel #143] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0307 14:38:04.185509       1 logging.go:59] [core] [Channel #172 SubChannel #173] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0307 14:38:04.188749       1 logging.go:59] [core] [Channel #82 SubChannel #83] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"

* 
* ==> kube-controller-manager [137bddb8edb6] <==
* I0308 01:38:15.330073       1 job_controller.go:514] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0308 01:38:15.362004       1 job_controller.go:514] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0308 01:38:15.362973       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-admission-patch" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-admission-patch-h982p"
I0308 01:38:15.516716       1 job_controller.go:514] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0308 01:38:15.533581       1 job_controller.go:514] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0308 01:38:15.546044       1 job_controller.go:514] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0308 01:38:16.026083       1 job_controller.go:514] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0308 01:38:20.158207       1 job_controller.go:514] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0308 01:38:20.184893       1 job_controller.go:514] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0308 01:38:21.521638       1 job_controller.go:514] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0308 01:38:22.545790       1 job_controller.go:514] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0308 01:38:22.636463       1 job_controller.go:514] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0308 01:38:23.561377       1 job_controller.go:514] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0308 01:38:23.596158       1 job_controller.go:514] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0308 01:38:23.612318       1 job_controller.go:514] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0308 01:38:23.620378       1 job_controller.go:514] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0308 01:38:23.620660       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-admission-create" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="Completed" message="Job completed"
I0308 01:38:23.648857       1 job_controller.go:514] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0308 01:38:23.660107       1 job_controller.go:514] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0308 01:38:23.670324       1 job_controller.go:514] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0308 01:38:23.670423       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-admission-patch" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="Completed" message="Job completed"
I0308 01:42:56.026308       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ingress-nginx-controller-c69664497 to 1"
I0308 01:42:56.235647       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller-c69664497" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-controller-c69664497-4t4h2"
I0308 01:43:17.318785       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set ingress-nginx-controller-77669ff58 to 0 from 1"
I0308 01:43:17.452194       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller-77669ff58" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: ingress-nginx-controller-77669ff58-tft2w"
I0308 01:43:18.624516       1 job_controller.go:514] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0308 01:43:18.635638       1 job_controller.go:514] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0308 01:45:06.850838       1 namespace_controller.go:180] Namespace has been deleted ingress-nginx
I0308 01:45:51.365709       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ingress-nginx-controller-c69664497 to 1"
I0308 01:45:51.380000       1 job_controller.go:514] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0308 01:45:51.415969       1 job_controller.go:514] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0308 01:45:51.416001       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-admission-create" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-admission-create-v6xwv"
I0308 01:45:51.416451       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller-c69664497" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-controller-c69664497-vczh8"
I0308 01:45:51.437881       1 job_controller.go:514] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0308 01:45:51.438062       1 job_controller.go:514] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0308 01:45:51.438155       1 job_controller.go:514] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0308 01:45:51.459398       1 job_controller.go:514] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0308 01:45:51.459901       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-admission-patch" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-admission-patch-zmfsb"
I0308 01:45:51.542620       1 job_controller.go:514] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0308 01:45:51.543968       1 job_controller.go:514] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0308 01:45:51.627927       1 job_controller.go:514] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0308 01:45:51.644825       1 job_controller.go:514] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0308 01:45:54.773554       1 job_controller.go:514] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0308 01:45:56.222209       1 job_controller.go:514] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0308 01:45:57.423828       1 job_controller.go:514] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0308 01:45:57.448435       1 job_controller.go:514] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0308 01:45:58.415113       1 job_controller.go:514] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0308 01:45:58.434063       1 job_controller.go:514] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0308 01:45:58.446480       1 job_controller.go:514] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0308 01:45:58.456775       1 job_controller.go:514] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0308 01:45:58.456903       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-admission-create" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="Completed" message="Job completed"
I0308 01:45:59.436883       1 job_controller.go:514] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0308 01:46:00.470261       1 job_controller.go:514] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0308 01:46:00.488895       1 job_controller.go:514] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0308 01:46:00.503368       1 job_controller.go:514] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0308 01:46:00.503772       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-admission-patch" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="Completed" message="Job completed"
I0308 01:46:11.334353       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ingress-nginx-controller-77669ff58 to 1"
I0308 01:46:11.350211       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller-77669ff58" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-controller-77669ff58-2fhgf"
I0308 01:46:11.736982       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set ingress-nginx-controller-c69664497 to 0 from 1"
I0308 01:46:11.845526       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller-c69664497" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: ingress-nginx-controller-c69664497-vczh8"

* 
* ==> kube-controller-manager [53be5b0bbcd0] <==
* I0307 12:02:37.327216       1 job_controller.go:514] enqueueing job ingress-nginx/ingress-nginx-admission-patch
W0307 12:02:37.329787       1 actual_state_of_world.go:541] Failed to update statusUpdateNeeded field in actual state of world: Failed to set statusUpdateNeeded to needed true, because nodeName="minikube" does not exist
I0307 12:02:37.329887       1 shared_informer.go:280] Caches are synced for cronjob
I0307 12:02:37.385531       1 shared_informer.go:280] Caches are synced for node
I0307 12:02:37.385763       1 range_allocator.go:167] Sending events to api server.
I0307 12:02:37.385876       1 range_allocator.go:171] Starting range CIDR allocator
I0307 12:02:37.385914       1 shared_informer.go:273] Waiting for caches to sync for cidrallocator
I0307 12:02:37.385970       1 shared_informer.go:280] Caches are synced for cidrallocator
I0307 12:02:37.386415       1 shared_informer.go:280] Caches are synced for PV protection
I0307 12:02:37.386529       1 shared_informer.go:280] Caches are synced for ClusterRoleAggregator
I0307 12:02:37.389467       1 shared_informer.go:280] Caches are synced for certificate-csrapproving
I0307 12:02:37.391220       1 shared_informer.go:280] Caches are synced for bootstrap_signer
I0307 12:02:37.407439       1 shared_informer.go:280] Caches are synced for expand
I0307 12:02:37.407558       1 shared_informer.go:280] Caches are synced for TTL after finished
I0307 12:02:37.407983       1 shared_informer.go:280] Caches are synced for TTL
I0307 12:02:37.408111       1 shared_informer.go:280] Caches are synced for service account
I0307 12:02:37.409952       1 shared_informer.go:280] Caches are synced for certificate-csrsigning-kubelet-serving
I0307 12:02:37.409958       1 shared_informer.go:280] Caches are synced for certificate-csrsigning-kubelet-client
I0307 12:02:37.410045       1 shared_informer.go:280] Caches are synced for certificate-csrsigning-legacy-unknown
I0307 12:02:37.409985       1 shared_informer.go:280] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0307 12:02:37.485081       1 shared_informer.go:280] Caches are synced for namespace
I0307 12:02:37.485411       1 shared_informer.go:280] Caches are synced for crt configmap
E0307 12:02:37.491088       1 memcache.go:255] couldn't get resource list for metrics.k8s.io/v1beta1: the server is currently unable to handle the request
I0307 12:02:37.507582       1 shared_informer.go:280] Caches are synced for endpoint_slice_mirroring
E0307 12:02:37.510450       1 memcache.go:106] couldn't get resource list for metrics.k8s.io/v1beta1: the server is currently unable to handle the request
I0307 12:02:37.520803       1 shared_informer.go:273] Waiting for caches to sync for garbage collector
I0307 12:02:37.606430       1 job_controller.go:514] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0307 12:02:37.608630       1 job_controller.go:514] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0307 12:02:37.608785       1 shared_informer.go:280] Caches are synced for deployment
I0307 12:02:37.613091       1 shared_informer.go:280] Caches are synced for resource quota
I0307 12:02:37.613227       1 shared_informer.go:280] Caches are synced for stateful set
I0307 12:02:37.613285       1 shared_informer.go:280] Caches are synced for job
I0307 12:02:37.613317       1 shared_informer.go:280] Caches are synced for persistent volume
I0307 12:02:37.613390       1 shared_informer.go:280] Caches are synced for HPA
I0307 12:02:37.613416       1 shared_informer.go:280] Caches are synced for ReplicationController
I0307 12:02:37.613792       1 shared_informer.go:280] Caches are synced for resource quota
I0307 12:02:37.614376       1 shared_informer.go:280] Caches are synced for disruption
I0307 12:02:37.630256       1 shared_informer.go:280] Caches are synced for GC
I0307 12:02:37.642433       1 shared_informer.go:280] Caches are synced for taint
I0307 12:02:37.642605       1 taint_manager.go:206] "Starting NoExecuteTaintManager"
I0307 12:02:37.642669       1 taint_manager.go:211] "Sending events to api server"
I0307 12:02:37.642624       1 node_lifecycle_controller.go:1438] Initializing eviction metric for zone: 
I0307 12:02:37.643236       1 event.go:294] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
W0307 12:02:37.643853       1 node_lifecycle_controller.go:1053] Missing timestamp for Node minikube. Assuming now as a timestamp.
I0307 12:02:37.643940       1 node_lifecycle_controller.go:1254] Controller detected that zone  is now in state Normal.
I0307 12:02:37.685136       1 shared_informer.go:280] Caches are synced for endpoint
I0307 12:02:37.687876       1 shared_informer.go:280] Caches are synced for ReplicaSet
I0307 12:02:37.691842       1 shared_informer.go:280] Caches are synced for ephemeral
I0307 12:02:37.698133       1 shared_informer.go:280] Caches are synced for attach detach
I0307 12:02:37.702241       1 shared_informer.go:280] Caches are synced for endpoint_slice
I0307 12:02:37.705989       1 shared_informer.go:280] Caches are synced for daemon sets
I0307 12:02:37.706733       1 shared_informer.go:280] Caches are synced for PVC protection
I0307 12:02:37.921252       1 shared_informer.go:280] Caches are synced for garbage collector
I0307 12:02:37.989870       1 shared_informer.go:280] Caches are synced for garbage collector
I0307 12:02:37.989989       1 garbagecollector.go:163] Garbage collector: all resource monitors have synced. Proceeding to collect garbage
I0307 13:46:01.216274       1 event.go:294] "Event occurred" object="default/database-persistent-volume-claim" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Normal" reason="ExternalProvisioning" message="waiting for a volume to be created, either by external provisioner \"k8s.io/minikube-hostpath\" or manually created by system administrator"
I0307 13:46:01.317464       1 event.go:294] "Event occurred" object="default/postgress-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set postgress-deployment-7fb56b4f4b to 1"
I0307 13:46:01.398236       1 event.go:294] "Event occurred" object="default/postgress-deployment-7fb56b4f4b" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: postgress-deployment-7fb56b4f4b-68kvz"
I0307 13:46:12.730469       1 event.go:294] "Event occurred" object="default/postgress-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set postgress-deployment-6df6dc4477 to 0 from 1"
I0307 13:46:12.902905       1 event.go:294] "Event occurred" object="default/postgress-deployment-6df6dc4477" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: postgress-deployment-6df6dc4477-989gn"

* 
* ==> kube-proxy [9bab411d217b] <==
* I0307 12:02:28.651848       1 node.go:163] Successfully retrieved node IP: 192.168.49.2
I0307 12:02:28.651976       1 server_others.go:109] "Detected node IP" address="192.168.49.2"
I0307 12:02:28.652051       1 server_others.go:535] "Using iptables proxy"
I0307 12:02:30.613660       1 server_others.go:176] "Using iptables Proxier"
I0307 12:02:30.614607       1 server_others.go:183] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I0307 12:02:30.614651       1 server_others.go:184] "Creating dualStackProxier for iptables"
I0307 12:02:30.687347       1 server_others.go:465] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, , defaulting to no-op detect-local for IPv6"
I0307 12:02:30.793890       1 proxier.go:242] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0307 12:02:30.886059       1 server.go:655] "Version info" version="v1.26.1"
I0307 12:02:30.886245       1 server.go:657] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0307 12:02:30.986674       1 config.go:317] "Starting service config controller"
I0307 12:02:30.986969       1 config.go:226] "Starting endpoint slice config controller"
I0307 12:02:30.987060       1 config.go:444] "Starting node config controller"
I0307 12:02:30.992245       1 shared_informer.go:273] Waiting for caches to sync for endpoint slice config
I0307 12:02:30.992440       1 shared_informer.go:273] Waiting for caches to sync for node config
I0307 12:02:30.992558       1 shared_informer.go:273] Waiting for caches to sync for service config
I0307 12:02:31.092725       1 shared_informer.go:280] Caches are synced for node config
I0307 12:02:31.092754       1 shared_informer.go:280] Caches are synced for endpoint slice config
I0307 12:02:31.093967       1 shared_informer.go:280] Caches are synced for service config

* 
* ==> kube-proxy [a60fa573a6bd] <==
* I0307 21:40:13.048356       1 node.go:163] Successfully retrieved node IP: 192.168.49.2
I0307 21:40:13.050091       1 server_others.go:109] "Detected node IP" address="192.168.49.2"
I0307 21:40:13.051188       1 server_others.go:535] "Using iptables proxy"
I0307 21:40:16.535284       1 server_others.go:176] "Using iptables Proxier"
I0307 21:40:16.535705       1 server_others.go:183] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I0307 21:40:16.535749       1 server_others.go:184] "Creating dualStackProxier for iptables"
I0307 21:40:16.536629       1 server_others.go:465] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, , defaulting to no-op detect-local for IPv6"
I0307 21:40:16.544405       1 proxier.go:242] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0307 21:40:16.550957       1 server.go:655] "Version info" version="v1.26.1"
I0307 21:40:16.550987       1 server.go:657] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0307 21:40:16.740005       1 config.go:317] "Starting service config controller"
I0307 21:40:16.741819       1 shared_informer.go:273] Waiting for caches to sync for service config
I0307 21:40:16.742042       1 config.go:226] "Starting endpoint slice config controller"
I0307 21:40:16.742054       1 shared_informer.go:273] Waiting for caches to sync for endpoint slice config
I0307 21:40:16.814553       1 config.go:444] "Starting node config controller"
I0307 21:40:16.814634       1 shared_informer.go:273] Waiting for caches to sync for node config
I0307 21:40:16.943118       1 shared_informer.go:280] Caches are synced for endpoint slice config
I0307 21:40:16.943120       1 shared_informer.go:280] Caches are synced for service config
I0307 21:40:17.015598       1 shared_informer.go:280] Caches are synced for node config
E0308 01:23:28.030699       1 service_health.go:187] "Healthcheck closed" err="accept tcp [::]:30260: use of closed network connection" service="ingress-nginx/ingress-nginx-controller"
E0308 01:27:12.133759       1 service_health.go:187] "Healthcheck closed" err="accept tcp [::]:30803: use of closed network connection" service="ingress-nginx/ingress-nginx-controller"
E0308 01:33:59.816802       1 service_health.go:187] "Healthcheck closed" err="accept tcp [::]:32605: use of closed network connection" service="ingress-nginx/ingress-nginx-controller"
E0308 01:43:18.714816       1 service_health.go:187] "Healthcheck closed" err="accept tcp [::]:31190: use of closed network connection" service="ingress-nginx/ingress-nginx-controller"
E0308 01:46:11.715460       1 service_health.go:187] "Healthcheck closed" err="accept tcp [::]:30937: use of closed network connection" service="ingress-nginx/ingress-nginx-controller"

* 
* ==> kube-scheduler [0655a87664f0] <==
* I0307 21:39:30.224830       1 serving.go:348] Generated self-signed cert in-memory
W0307 21:39:34.334743       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0307 21:39:34.335517       1 authentication.go:349] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0307 21:39:34.335683       1 authentication.go:350] Continuing without authentication configuration. This may treat all requests as anonymous.
W0307 21:39:34.335810       1 authentication.go:351] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0307 21:39:34.529737       1 server.go:152] "Starting Kubernetes Scheduler" version="v1.26.1"
I0307 21:39:34.529797       1 server.go:154] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0307 21:39:34.537807       1 secure_serving.go:210] Serving securely on 127.0.0.1:10259
I0307 21:39:34.537949       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0307 21:39:34.538162       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0307 21:39:34.539105       1 shared_informer.go:273] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0307 21:39:34.639614       1 shared_informer.go:280] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kube-scheduler [b243e3e46b2d] <==
* I0307 12:02:13.500140       1 serving.go:348] Generated self-signed cert in-memory
W0307 12:02:17.106614       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0307 12:02:17.107011       1 authentication.go:349] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0307 12:02:17.107182       1 authentication.go:350] Continuing without authentication configuration. This may treat all requests as anonymous.
W0307 12:02:17.107373       1 authentication.go:351] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0307 12:02:17.555985       1 server.go:152] "Starting Kubernetes Scheduler" version="v1.26.1"
I0307 12:02:17.556095       1 server.go:154] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0307 12:02:17.572115       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0307 12:02:17.572175       1 shared_informer.go:273] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0307 12:02:17.572358       1 secure_serving.go:210] Serving securely on 127.0.0.1:10259
I0307 12:02:17.572485       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0307 12:02:17.685380       1 shared_informer.go:280] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0307 14:38:02.703179       1 secure_serving.go:255] Stopped listening on 127.0.0.1:10259
E0307 14:38:02.704348       1 run.go:74] "command failed" err="finished without leader elect"
I0307 14:38:02.704456       1 tlsconfig.go:255] "Shutting down DynamicServingCertificateController"

* 
* ==> kubelet <==
* -- Logs begin at Tue 2023-03-07 21:38:50 UTC, end at Wed 2023-03-08 01:50:37 UTC. --
Mar 08 01:43:53 minikube kubelet[1587]: I0308 01:43:53.677489    1587 scope.go:115] "RemoveContainer" containerID="c4c971cc9ddb25c509186d0ab7d3807ef10269cd3c3bcfadc5b817806b4c3515"
Mar 08 01:43:53 minikube kubelet[1587]: I0308 01:43:53.708054    1587 scope.go:115] "RemoveContainer" containerID="fe30ca798e81425cf3dd8bdc3e7e19e9ba8a45a1e727ca501e436a4d8fb8c960"
Mar 08 01:44:20 minikube kubelet[1587]: W0308 01:44:20.918768    1587 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Mar 08 01:45:51 minikube kubelet[1587]: I0308 01:45:51.429808    1587 topology_manager.go:210] "Topology Admit Handler"
Mar 08 01:45:51 minikube kubelet[1587]: E0308 01:45:51.429938    1587 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="59bb4f31-f66f-4d0f-8245-9e278a0f6fb1" containerName="controller"
Mar 08 01:45:51 minikube kubelet[1587]: E0308 01:45:51.429962    1587 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="b102ff3a-ba87-4ae9-8be4-29d3034e10f4" containerName="controller"
Mar 08 01:45:51 minikube kubelet[1587]: E0308 01:45:51.429972    1587 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="d215ca6b-2262-404c-9d35-251c011a588f" containerName="patch"
Mar 08 01:45:51 minikube kubelet[1587]: I0308 01:45:51.430016    1587 memory_manager.go:346] "RemoveStaleState removing state" podUID="b102ff3a-ba87-4ae9-8be4-29d3034e10f4" containerName="controller"
Mar 08 01:45:51 minikube kubelet[1587]: I0308 01:45:51.430028    1587 memory_manager.go:346] "RemoveStaleState removing state" podUID="59bb4f31-f66f-4d0f-8245-9e278a0f6fb1" containerName="controller"
Mar 08 01:45:51 minikube kubelet[1587]: I0308 01:45:51.437239    1587 topology_manager.go:210] "Topology Admit Handler"
Mar 08 01:45:51 minikube kubelet[1587]: I0308 01:45:51.534910    1587 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-bjcg8\" (UniqueName: \"kubernetes.io/projected/161617a9-c5f5-441c-ace6-28021c559de7-kube-api-access-bjcg8\") pod \"ingress-nginx-controller-c69664497-vczh8\" (UID: \"161617a9-c5f5-441c-ace6-28021c559de7\") " pod="ingress-nginx/ingress-nginx-controller-c69664497-vczh8"
Mar 08 01:45:51 minikube kubelet[1587]: I0308 01:45:51.535189    1587 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"webhook-cert\" (UniqueName: \"kubernetes.io/secret/161617a9-c5f5-441c-ace6-28021c559de7-webhook-cert\") pod \"ingress-nginx-controller-c69664497-vczh8\" (UID: \"161617a9-c5f5-441c-ace6-28021c559de7\") " pod="ingress-nginx/ingress-nginx-controller-c69664497-vczh8"
Mar 08 01:45:51 minikube kubelet[1587]: I0308 01:45:51.536564    1587 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-dbwdt\" (UniqueName: \"kubernetes.io/projected/dbfca0da-fd9c-43fc-aaf8-f34ebecb1f5c-kube-api-access-dbwdt\") pod \"ingress-nginx-admission-create-v6xwv\" (UID: \"dbfca0da-fd9c-43fc-aaf8-f34ebecb1f5c\") " pod="ingress-nginx/ingress-nginx-admission-create-v6xwv"
Mar 08 01:45:51 minikube kubelet[1587]: I0308 01:45:51.613950    1587 topology_manager.go:210] "Topology Admit Handler"
Mar 08 01:45:51 minikube kubelet[1587]: E0308 01:45:51.638247    1587 secret.go:194] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Mar 08 01:45:51 minikube kubelet[1587]: E0308 01:45:51.638402    1587 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/161617a9-c5f5-441c-ace6-28021c559de7-webhook-cert podName:161617a9-c5f5-441c-ace6-28021c559de7 nodeName:}" failed. No retries permitted until 2023-03-08 01:45:52.1383777 +0000 UTC m=+14792.428160601 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/161617a9-c5f5-441c-ace6-28021c559de7-webhook-cert") pod "ingress-nginx-controller-c69664497-vczh8" (UID: "161617a9-c5f5-441c-ace6-28021c559de7") : secret "ingress-nginx-admission" not found
Mar 08 01:45:51 minikube kubelet[1587]: I0308 01:45:51.739915    1587 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-t528s\" (UniqueName: \"kubernetes.io/projected/b6ac98d7-5579-4387-be47-3400d2e4d0f6-kube-api-access-t528s\") pod \"ingress-nginx-admission-patch-zmfsb\" (UID: \"b6ac98d7-5579-4387-be47-3400d2e4d0f6\") " pod="ingress-nginx/ingress-nginx-admission-patch-zmfsb"
Mar 08 01:45:51 minikube kubelet[1587]: I0308 01:45:51.953385    1587 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="5f82a85d1589c1ab744e12dc3c3b7b1502a91a488249792af3f5e8f32e8423f2"
Mar 08 01:45:52 minikube kubelet[1587]: E0308 01:45:52.147149    1587 secret.go:194] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Mar 08 01:45:52 minikube kubelet[1587]: E0308 01:45:52.147227    1587 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/161617a9-c5f5-441c-ace6-28021c559de7-webhook-cert podName:161617a9-c5f5-441c-ace6-28021c559de7 nodeName:}" failed. No retries permitted until 2023-03-08 01:45:53.1472125 +0000 UTC m=+14793.436995401 (durationBeforeRetry 1s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/161617a9-c5f5-441c-ace6-28021c559de7-webhook-cert") pod "ingress-nginx-controller-c69664497-vczh8" (UID: "161617a9-c5f5-441c-ace6-28021c559de7") : secret "ingress-nginx-admission" not found
Mar 08 01:45:53 minikube kubelet[1587]: I0308 01:45:53.053606    1587 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="8d3053dbe1282a560be8fe6c7880a1980e208cd4f17d4c17d3be7a97a4679e55"
Mar 08 01:45:53 minikube kubelet[1587]: E0308 01:45:53.217895    1587 secret.go:194] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Mar 08 01:45:53 minikube kubelet[1587]: E0308 01:45:53.218721    1587 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/161617a9-c5f5-441c-ace6-28021c559de7-webhook-cert podName:161617a9-c5f5-441c-ace6-28021c559de7 nodeName:}" failed. No retries permitted until 2023-03-08 01:45:55.218109 +0000 UTC m=+14795.507892101 (durationBeforeRetry 2s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/161617a9-c5f5-441c-ace6-28021c559de7-webhook-cert") pod "ingress-nginx-controller-c69664497-vczh8" (UID: "161617a9-c5f5-441c-ace6-28021c559de7") : secret "ingress-nginx-admission" not found
Mar 08 01:45:56 minikube kubelet[1587]: I0308 01:45:56.028718    1587 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="d269c38b3b3e413ba220fc911bd86764a6d0a868f723775caabf98b1f3547f55"
Mar 08 01:45:56 minikube kubelet[1587]: I0308 01:45:56.200145    1587 scope.go:115] "RemoveContainer" containerID="346f91febcecbb1bbd2999bf45b39cdd0b2bdf28fbce1f5af7d51dd64bc42b7c"
Mar 08 01:45:56 minikube kubelet[1587]: I0308 01:45:56.979562    1587 reconciler_common.go:169] "operationExecutor.UnmountVolume started for volume \"kube-api-access-dbwdt\" (UniqueName: \"kubernetes.io/projected/dbfca0da-fd9c-43fc-aaf8-f34ebecb1f5c-kube-api-access-dbwdt\") pod \"dbfca0da-fd9c-43fc-aaf8-f34ebecb1f5c\" (UID: \"dbfca0da-fd9c-43fc-aaf8-f34ebecb1f5c\") "
Mar 08 01:45:56 minikube kubelet[1587]: I0308 01:45:56.984177    1587 operation_generator.go:890] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/dbfca0da-fd9c-43fc-aaf8-f34ebecb1f5c-kube-api-access-dbwdt" (OuterVolumeSpecName: "kube-api-access-dbwdt") pod "dbfca0da-fd9c-43fc-aaf8-f34ebecb1f5c" (UID: "dbfca0da-fd9c-43fc-aaf8-f34ebecb1f5c"). InnerVolumeSpecName "kube-api-access-dbwdt". PluginName "kubernetes.io/projected", VolumeGidValue ""
Mar 08 01:45:57 minikube kubelet[1587]: I0308 01:45:57.080343    1587 reconciler_common.go:295] "Volume detached for volume \"kube-api-access-dbwdt\" (UniqueName: \"kubernetes.io/projected/dbfca0da-fd9c-43fc-aaf8-f34ebecb1f5c-kube-api-access-dbwdt\") on node \"minikube\" DevicePath \"\""
Mar 08 01:45:57 minikube kubelet[1587]: I0308 01:45:57.270326    1587 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="5f82a85d1589c1ab744e12dc3c3b7b1502a91a488249792af3f5e8f32e8423f2"
Mar 08 01:45:57 minikube kubelet[1587]: I0308 01:45:57.316605    1587 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="ingress-nginx/ingress-nginx-controller-c69664497-vczh8" podStartSLOduration=6.3165485 pod.CreationTimestamp="2023-03-08 01:45:51 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2023-03-08 01:45:57.3158232 +0000 UTC m=+14797.605606201" watchObservedRunningTime="2023-03-08 01:45:57.3165485 +0000 UTC m=+14797.606331401"
Mar 08 01:45:57 minikube kubelet[1587]: I0308 01:45:57.349873    1587 scope.go:115] "RemoveContainer" containerID="346f91febcecbb1bbd2999bf45b39cdd0b2bdf28fbce1f5af7d51dd64bc42b7c"
Mar 08 01:45:59 minikube kubelet[1587]: I0308 01:45:59.234355    1587 reconciler_common.go:169] "operationExecutor.UnmountVolume started for volume \"kube-api-access-t528s\" (UniqueName: \"kubernetes.io/projected/b6ac98d7-5579-4387-be47-3400d2e4d0f6-kube-api-access-t528s\") pod \"b6ac98d7-5579-4387-be47-3400d2e4d0f6\" (UID: \"b6ac98d7-5579-4387-be47-3400d2e4d0f6\") "
Mar 08 01:45:59 minikube kubelet[1587]: I0308 01:45:59.237516    1587 operation_generator.go:890] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/b6ac98d7-5579-4387-be47-3400d2e4d0f6-kube-api-access-t528s" (OuterVolumeSpecName: "kube-api-access-t528s") pod "b6ac98d7-5579-4387-be47-3400d2e4d0f6" (UID: "b6ac98d7-5579-4387-be47-3400d2e4d0f6"). InnerVolumeSpecName "kube-api-access-t528s". PluginName "kubernetes.io/projected", VolumeGidValue ""
Mar 08 01:45:59 minikube kubelet[1587]: I0308 01:45:59.334653    1587 reconciler_common.go:295] "Volume detached for volume \"kube-api-access-t528s\" (UniqueName: \"kubernetes.io/projected/b6ac98d7-5579-4387-be47-3400d2e4d0f6-kube-api-access-t528s\") on node \"minikube\" DevicePath \"\""
Mar 08 01:45:59 minikube kubelet[1587]: I0308 01:45:59.416267    1587 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="8d3053dbe1282a560be8fe6c7880a1980e208cd4f17d4c17d3be7a97a4679e55"
Mar 08 01:46:11 minikube kubelet[1587]: I0308 01:46:11.820445    1587 topology_manager.go:210] "Topology Admit Handler"
Mar 08 01:46:11 minikube kubelet[1587]: E0308 01:46:11.821987    1587 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="b6ac98d7-5579-4387-be47-3400d2e4d0f6" containerName="patch"
Mar 08 01:46:11 minikube kubelet[1587]: E0308 01:46:11.822073    1587 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="dbfca0da-fd9c-43fc-aaf8-f34ebecb1f5c" containerName="create"
Mar 08 01:46:11 minikube kubelet[1587]: I0308 01:46:11.822160    1587 memory_manager.go:346] "RemoveStaleState removing state" podUID="dbfca0da-fd9c-43fc-aaf8-f34ebecb1f5c" containerName="create"
Mar 08 01:46:11 minikube kubelet[1587]: I0308 01:46:11.822177    1587 memory_manager.go:346] "RemoveStaleState removing state" podUID="b6ac98d7-5579-4387-be47-3400d2e4d0f6" containerName="patch"
Mar 08 01:46:11 minikube kubelet[1587]: I0308 01:46:11.822193    1587 memory_manager.go:346] "RemoveStaleState removing state" podUID="b6ac98d7-5579-4387-be47-3400d2e4d0f6" containerName="patch"
Mar 08 01:46:11 minikube kubelet[1587]: I0308 01:46:11.939065    1587 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"webhook-cert\" (UniqueName: \"kubernetes.io/secret/cbbf56c2-26d4-4f90-a0bf-f9f9be959f95-webhook-cert\") pod \"ingress-nginx-controller-77669ff58-2fhgf\" (UID: \"cbbf56c2-26d4-4f90-a0bf-f9f9be959f95\") " pod="ingress-nginx/ingress-nginx-controller-77669ff58-2fhgf"
Mar 08 01:46:11 minikube kubelet[1587]: I0308 01:46:11.939134    1587 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-ldt4c\" (UniqueName: \"kubernetes.io/projected/cbbf56c2-26d4-4f90-a0bf-f9f9be959f95-kube-api-access-ldt4c\") pod \"ingress-nginx-controller-77669ff58-2fhgf\" (UID: \"cbbf56c2-26d4-4f90-a0bf-f9f9be959f95\") " pod="ingress-nginx/ingress-nginx-controller-77669ff58-2fhgf"
Mar 08 01:46:13 minikube kubelet[1587]: I0308 01:46:13.467963    1587 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="90c1b7d583748153817ccea4e3db73f3a2140c8920761deb2d2f76c76dc0633c"
Mar 08 01:46:14 minikube kubelet[1587]: E0308 01:46:14.515048    1587 remote_runtime.go:415] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error: No such container: e519b653bab510432f3bf14d67627856c1e47f61efdda12896e6cb43a569d160" containerID="e519b653bab510432f3bf14d67627856c1e47f61efdda12896e6cb43a569d160"
Mar 08 01:46:14 minikube kubelet[1587]: E0308 01:46:14.515614    1587 kuberuntime_manager.go:1085] "getPodContainerStatuses for pod failed" err="rpc error: code = Unknown desc = Error: No such container: e519b653bab510432f3bf14d67627856c1e47f61efdda12896e6cb43a569d160" pod="ingress-nginx/ingress-nginx-controller-77669ff58-2fhgf"
Mar 08 01:46:14 minikube kubelet[1587]: E0308 01:46:14.515747    1587 generic.go:449] "PLEG: Write status" err="rpc error: code = Unknown desc = Error: No such container: e519b653bab510432f3bf14d67627856c1e47f61efdda12896e6cb43a569d160" pod="ingress-nginx/ingress-nginx-controller-77669ff58-2fhgf"
Mar 08 01:46:24 minikube kubelet[1587]: I0308 01:46:24.294107    1587 reconciler_common.go:169] "operationExecutor.UnmountVolume started for volume \"kube-api-access-bjcg8\" (UniqueName: \"kubernetes.io/projected/161617a9-c5f5-441c-ace6-28021c559de7-kube-api-access-bjcg8\") pod \"161617a9-c5f5-441c-ace6-28021c559de7\" (UID: \"161617a9-c5f5-441c-ace6-28021c559de7\") "
Mar 08 01:46:24 minikube kubelet[1587]: I0308 01:46:24.294179    1587 reconciler_common.go:169] "operationExecutor.UnmountVolume started for volume \"webhook-cert\" (UniqueName: \"kubernetes.io/secret/161617a9-c5f5-441c-ace6-28021c559de7-webhook-cert\") pod \"161617a9-c5f5-441c-ace6-28021c559de7\" (UID: \"161617a9-c5f5-441c-ace6-28021c559de7\") "
Mar 08 01:46:24 minikube kubelet[1587]: I0308 01:46:24.296663    1587 operation_generator.go:890] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/161617a9-c5f5-441c-ace6-28021c559de7-webhook-cert" (OuterVolumeSpecName: "webhook-cert") pod "161617a9-c5f5-441c-ace6-28021c559de7" (UID: "161617a9-c5f5-441c-ace6-28021c559de7"). InnerVolumeSpecName "webhook-cert". PluginName "kubernetes.io/secret", VolumeGidValue ""
Mar 08 01:46:24 minikube kubelet[1587]: I0308 01:46:24.296883    1587 operation_generator.go:890] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/161617a9-c5f5-441c-ace6-28021c559de7-kube-api-access-bjcg8" (OuterVolumeSpecName: "kube-api-access-bjcg8") pod "161617a9-c5f5-441c-ace6-28021c559de7" (UID: "161617a9-c5f5-441c-ace6-28021c559de7"). InnerVolumeSpecName "kube-api-access-bjcg8". PluginName "kubernetes.io/projected", VolumeGidValue ""
Mar 08 01:46:24 minikube kubelet[1587]: I0308 01:46:24.394846    1587 reconciler_common.go:295] "Volume detached for volume \"kube-api-access-bjcg8\" (UniqueName: \"kubernetes.io/projected/161617a9-c5f5-441c-ace6-28021c559de7-kube-api-access-bjcg8\") on node \"minikube\" DevicePath \"\""
Mar 08 01:46:24 minikube kubelet[1587]: I0308 01:46:24.395121    1587 reconciler_common.go:295] "Volume detached for volume \"webhook-cert\" (UniqueName: \"kubernetes.io/secret/161617a9-c5f5-441c-ace6-28021c559de7-webhook-cert\") on node \"minikube\" DevicePath \"\""
Mar 08 01:46:24 minikube kubelet[1587]: I0308 01:46:24.615942    1587 scope.go:115] "RemoveContainer" containerID="4e66457ad3d76f833c9e34b5e94836e3ec21ac9701084468e79af74a12393425"
Mar 08 01:46:24 minikube kubelet[1587]: I0308 01:46:24.634763    1587 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="ingress-nginx/ingress-nginx-controller-77669ff58-2fhgf" podStartSLOduration=13.6346775 pod.CreationTimestamp="2023-03-08 01:46:11 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2023-03-08 01:46:16.3180863 +0000 UTC m=+14816.607869301" watchObservedRunningTime="2023-03-08 01:46:24.6346775 +0000 UTC m=+14824.922866101"
Mar 08 01:46:24 minikube kubelet[1587]: I0308 01:46:24.656736    1587 scope.go:115] "RemoveContainer" containerID="4e66457ad3d76f833c9e34b5e94836e3ec21ac9701084468e79af74a12393425"
Mar 08 01:46:24 minikube kubelet[1587]: E0308 01:46:24.658172    1587 remote_runtime.go:415] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error: No such container: 4e66457ad3d76f833c9e34b5e94836e3ec21ac9701084468e79af74a12393425" containerID="4e66457ad3d76f833c9e34b5e94836e3ec21ac9701084468e79af74a12393425"
Mar 08 01:46:24 minikube kubelet[1587]: I0308 01:46:24.658257    1587 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={Type:docker ID:4e66457ad3d76f833c9e34b5e94836e3ec21ac9701084468e79af74a12393425} err="failed to get container status \"4e66457ad3d76f833c9e34b5e94836e3ec21ac9701084468e79af74a12393425\": rpc error: code = Unknown desc = Error: No such container: 4e66457ad3d76f833c9e34b5e94836e3ec21ac9701084468e79af74a12393425"
Mar 08 01:46:25 minikube kubelet[1587]: I0308 01:46:25.988350    1587 kubelet_volumes.go:160] "Cleaned up orphaned pod volumes dir" podUID=161617a9-c5f5-441c-ace6-28021c559de7 path="/var/lib/kubelet/pods/161617a9-c5f5-441c-ace6-28021c559de7/volumes"
Mar 08 01:49:20 minikube kubelet[1587]: W0308 01:49:20.849159    1587 sysinfo.go:203] Nodes topology is not available, providing CPU topology

* 
* ==> kubernetes-dashboard [4a8313a5b5cd] <==
* 2023/03/07 21:40:36 Starting overwatch
2023/03/07 21:40:36 Using namespace: kubernetes-dashboard
2023/03/07 21:40:36 Using in-cluster config to connect to apiserver
2023/03/07 21:40:36 Using secret token for csrf signing
2023/03/07 21:40:36 Initializing csrf token from kubernetes-dashboard-csrf secret
2023/03/07 21:40:36 Empty token. Generating and storing in a secret kubernetes-dashboard-csrf
2023/03/07 21:40:36 Successful initial request to the apiserver, version: v1.26.1
2023/03/07 21:40:36 Generating JWE encryption key
2023/03/07 21:40:36 New synchronizer has been registered: kubernetes-dashboard-key-holder-kubernetes-dashboard. Starting
2023/03/07 21:40:36 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2023/03/07 21:40:36 Initializing JWE encryption key from synchronized object
2023/03/07 21:40:36 Creating in-cluster Sidecar client
2023/03/07 21:40:36 Successful request to sidecar
2023/03/07 21:40:36 Serving insecurely on HTTP port: 9090

* 
* ==> kubernetes-dashboard [f20c1d258a4d] <==
* 2023/03/07 21:40:16 Using namespace: kubernetes-dashboard
2023/03/07 21:40:16 Using in-cluster config to connect to apiserver
2023/03/07 21:40:16 Using secret token for csrf signing
2023/03/07 21:40:16 Initializing csrf token from kubernetes-dashboard-csrf secret
2023/03/07 21:40:16 Starting overwatch
panic: Get "https://10.96.0.1:443/api/v1/namespaces/kubernetes-dashboard/secrets/kubernetes-dashboard-csrf": x509: certificate signed by unknown authority

goroutine 1 [running]:
github.com/kubernetes/dashboard/src/app/backend/client/csrf.(*csrfTokenManager).init(0xc00061fae8)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/csrf/manager.go:41 +0x30e
github.com/kubernetes/dashboard/src/app/backend/client/csrf.NewCsrfTokenManager(...)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/csrf/manager.go:66
github.com/kubernetes/dashboard/src/app/backend/client.(*clientManager).initCSRFKey(0xc000256080)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:527 +0x94
github.com/kubernetes/dashboard/src/app/backend/client.(*clientManager).init(0x19aba3a?)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:495 +0x32
github.com/kubernetes/dashboard/src/app/backend/client.NewClientManager(...)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:594
main.main()
	/home/runner/work/dashboard/dashboard/src/app/backend/dashboard.go:96 +0x1cf

* 
* ==> storage-provisioner [2c19c781076c] <==
* I0307 21:40:34.238275       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0307 21:40:34.269407       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0307 21:40:34.272362       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0307 21:40:51.774984       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0307 21:40:51.775656       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_c94bbd11-2fff-47ac-8281-e0f8a646c150!
I0307 21:40:51.775675       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"3c16b23a-e5fc-4c1b-a480-67eb388ebdb6", APIVersion:"v1", ResourceVersion:"22180", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_c94bbd11-2fff-47ac-8281-e0f8a646c150 became leader
I0307 21:40:51.877763       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_c94bbd11-2fff-47ac-8281-e0f8a646c150!

* 
* ==> storage-provisioner [c58103e28ec7] <==
* I0307 21:40:12.522201       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0307 21:40:12.729861       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": x509: certificate signed by unknown authority

